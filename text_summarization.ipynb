{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "speech = \"\"\"\n",
    "I gave my first speech in May, 2010.\n",
    "It was at my hometown public library and  half a dozen people came out. I just kind of stood there and rambled. There were a lot of awkward silences. One guy clapped. \n",
    "Since then I’ve given over 500 keynote speeches. I am still getting better. I have a long way to go. One thing I’ve always done is collect speech transcripts and bits of poetry. I study them — inhale them — to see greatness in action. How do they build connection? How do they structure their messages? What’s the science? What’s the art? \n",
    "This is a page of my favorite speech transcripts and poetry: \n",
    "“What I regret most in my life are failures of kindness” by George Saunders\n",
    "Mississippi Testimony by Brandon Boulware\n",
    "“What, to the slave, is the fourth of July?” by Frederick Douglass\n",
    "Post-Game Speech on Jacob Blake and Culture of Fear by Coach Doc Rivers\n",
    "“The Other America” by Dr Martin Luther King Jr\n",
    "“I’ve Been to the Mountaintop” by Martin Luther King Jr\n",
    "Think Like a Bronze Medalist, not Silver by Derek Sivers\n",
    "On Children by Khalil Gibran\n",
    "“68 bits of unsolicited advice” by Kevin Kelly\n",
    "Television by Roald Dahl\n",
    "I believe in superheroes by IN-Q\n",
    "“Why did I say “yes” to speak here” by Malcolm Gladwell\n",
    "2020 Screen Actors Guild Acceptance Speech by Brad Pitt\n",
    "“What does why mean” by Richard Feynman\n",
    "“Invent your own life’s meaning” by Bill Watterson\n",
    "When Death Comes by Mary Oliver\n",
    "Pale Blue Dot by Carl Sagan\n",
    "Press conference answer on ‘long snapping’ by Bill Belichick\n",
    "If— by Rudyard Kipling\n",
    "“We don’t move on from grief. We move forward with it” by Nora McInerny\n",
    "To Laugh Much and Often by Bessie Anderson Stanley\n",
    "Blessing for the New Year by Kayleen Asbo\n",
    "Nobel Acceptance Speech by John Steinbeck\n",
    "You Learn by Anonymous\n",
    "“On the soul-sustaining necessity of resisting self-comparison and fighting cynicism” by Maria Popova\n",
    "Maximus, to himself by Charles Olson\n",
    "New Day’s Lyric by Amanda Gorman\n",
    "By the Well of Living and Seeing, Part II, Section 28: During the Second World War by Charles Reznikoff\n",
    "Be Drunk by Charles Baudelaire\n",
    "Quotes On Reading by Marcel Proust\n",
    "Two powerful songs about fatherhood by Harry Chapin & John Lennon\n",
    "Sauntering by Christopher Morley\n",
    "There is no Frigate Like a Book by Emily Dickinson\n",
    "Dreamland by Mark Twain\n",
    "“You Are Brilliant and the Earth is Hiring” by Paul Hawken\n",
    "What You Missed That Day You Were Absent From Fourth Grade by Brad Modlin\n",
    "The Sun by Mary Oliver\n",
    "Dust If You Must by Rose Milligan\n",
    "Don’t Hesitate by Mary Oliver\n",
    "Mother to Son by Langston Hughes\n",
    "Do not ask your children to strive by William Martin\n",
    "The Mind of Absolute Trust by Seng-ts’an\n",
    "Watching the Wheels by John Lennon\n",
    "Sparrow Envy by J. Drew Lanham\n",
    "‘A better measure for society’ by Robert F. Kennedy\n",
    "Leisure by W.H. Davies\n",
    "\"The right to the future tense\" by Shoshana Zuboff\n",
    "“The Anatomy of Trust” by Brené Brown\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model = \"llama3-8b-8192\", groq_api_key = groq_api_key )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nI gave my first speech in May, 2010.\\nIt was at my hometown public library and  half a dozen people came out. I just kind of stood there and rambled. There were a lot of awkward silences. One guy clapped. \\nSince then I’ve given over 500 keynote speeches. I am still getting better. I have a long way to go. One thing I’ve always done is collect speech transcripts and bits of poetry. I study them — inhale them — to see greatness in action. How do they build connection? How do they structure their messages? What’s the science? What’s the art? \\nThis is a page of my favorite speech transcripts and poetry: \\n“What I regret most in my life are failures of kindness” by George Saunders\\nMississippi Testimony by Brandon Boulware\\n“What, to the slave, is the fourth of July?” by Frederick Douglass\\nPost-Game Speech on Jacob Blake and Culture of Fear by Coach Doc Rivers\\n“The Other America” by Dr Martin Luther King Jr\\n“I’ve Been to the Mountaintop” by Martin Luther King Jr\\nThink Like a Bronze Medalist, not Silver by Derek Sivers\\nOn Children by Khalil Gibran\\n“68 bits of unsolicited advice” by Kevin Kelly\\nTelevision by Roald Dahl\\nI believe in superheroes by IN-Q\\n“Why did I say “yes” to speak here” by Malcolm Gladwell\\n2020 Screen Actors Guild Acceptance Speech by Brad Pitt\\n“What does why mean” by Richard Feynman\\n“Invent your own life’s meaning” by Bill Watterson\\nWhen Death Comes by Mary Oliver\\nPale Blue Dot by Carl Sagan\\nPress conference answer on ‘long snapping’ by Bill Belichick\\nIf— by Rudyard Kipling\\n“We don’t move on from grief. We move forward with it” by Nora McInerny\\nTo Laugh Much and Often by Bessie Anderson Stanley\\nBlessing for the New Year by Kayleen Asbo\\nNobel Acceptance Speech by John Steinbeck\\nYou Learn by Anonymous\\n“On the soul-sustaining necessity of resisting self-comparison and fighting cynicism” by Maria Popova\\nMaximus, to himself by Charles Olson\\nNew Day’s Lyric by Amanda Gorman\\nBy the Well of Living and Seeing, Part II, Section 28: During the Second World War by Charles Reznikoff\\nBe Drunk by Charles Baudelaire\\nQuotes On Reading by Marcel Proust\\nTwo powerful songs about fatherhood by Harry Chapin & John Lennon\\nSauntering by Christopher Morley\\nThere is no Frigate Like a Book by Emily Dickinson\\nDreamland by Mark Twain\\n“You Are Brilliant and the Earth is Hiring” by Paul Hawken\\nWhat You Missed That Day You Were Absent From Fourth Grade by Brad Modlin\\nThe Sun by Mary Oliver\\nDust If You Must by Rose Milligan\\nDon’t Hesitate by Mary Oliver\\nMother to Son by Langston Hughes\\nDo not ask your children to strive by William Martin\\nThe Mind of Absolute Trust by Seng-ts’an\\nWatching the Wheels by John Lennon\\nSparrow Envy by J. Drew Lanham\\n‘A better measure for society’ by Robert F. Kennedy\\nLeisure by W.H. Davies\\n\"The right to the future tense\" by Shoshana Zuboff\\n“The Anatomy of Trust” by Brené Brown\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_message = [\n",
    "    SystemMessage(content = \"You are expert with expertise in summarizing the speeches\"),\n",
    "    HumanMessage(content = f\"Please provide a short and concise summary of the following speech:\\n Text:{speech}\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\GEN AI\\Text-Summarization\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "773"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVA\\AppData\\Local\\Temp\\ipykernel_12060\\2571016473.py:1: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm(chat_message).content\n"
     ]
    }
   ],
   "source": [
    "response = llm(chat_message).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['language', 'num_of_tokens', 'speech'], input_types={}, partial_variables={}, template='\\nWrite a summary of the following speech:\\nspeech : {speech}\\nTranslate the precise summary to {language} language\\nBe sure to use atmost {num_of_tokens} tokens after summarizing, if not possible tell \\nthat the summarization is not possible with the given number of tokens\\n')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## creating a prompt template\n",
    "from langchain.chains import LLMChain ## connecting a model with a prompt\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template_string = \"\"\"\n",
    "Write a summary of the following speech:\n",
    "speech : {speech}\n",
    "Translate the precise summary to {language} language\n",
    "Be sure to use atmost {num_of_tokens} tokens after summarizing, if not possible tell \n",
    "that the summarization is not possible with the given number of tokens\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = [\"speech\", \"language\", \"num_of_tokens\"],\n",
    "    template = template_string\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt = prompt.format(speech = speech, language = \"tamil\", num_of_tokens = \"100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "829"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVA\\AppData\\Local\\Temp\\ipykernel_12060\\286185093.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(prompt = prompt, llm = llm)\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(prompt = prompt, llm = llm)\n",
    "summary = chain.invoke({\"speech\":speech, \"language\":\"french\",\"num_of_tokens\":200})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Summary:\\n\\nThe speaker reflects on their journey from giving their first speech in 2010 to giving over 500 keynote speeches since then. They have always collected and studied speech transcripts and poetry to understand how to build connection and structure their messages. The speaker shares their favorite quotes and excerpts from various speeches and poems, highlighting the importance of kindness, perseverance, and self-improvement.\\n\\nTranslation to French:\\n\\nLe speaker réfléchit à son parcours, depuis son premier discours en 2010 jusqu'à plus de 500 discours clés depuis lors. Ils ont toujours collecté et étudié les transcriptions de discours et les poèmes pour comprendre comment établir des liens et structurer leurs messages. Le speaker partage ses citations préférées et extraits de divers discours et poèmes, mettant en valeur l'importance de la gentillesse, de la persévérance et de l'amélioration de soi.\\n\\nNote: The summary is around 176 tokens, which is within the limit of 200 tokens.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Stuff document chain\n",
    "#### 2. map reduce summarization --> important for larger file \n",
    "1. Single prompt\n",
    "2. Multi prompt\n",
    "#### 3. Refine chain summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stuff document chain : \n",
    "##### PDF(Text documents) -> prompt template (do the summarization) -> LLM -> O/P\n",
    "But if the number of documents are large, it ll get combined and sent to prompt\n",
    "Challenges here is that : If there are more than thousand documents (very large), it is not right to send it directly to Prompt because of the limiting context of the LLM models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map reduce summarization : \n",
    "##### PDF(Text documents) -> divide into smaller chunks -> prompt template -> LLM model -> summary 1, summary 2.. -> combine all the summary -> final summary\n",
    "We can have 2 prompt templates one for every smaller summary and the other for final summary (it can be useful in case if we want to know some overall characteristic of the context summary like title)\n",
    "The process is all about reducing the context and combine it later.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refine Chain summarization : \n",
    "##### PDF(Text documents) ->split into many smaller document chunks -> prompt template1 (chunk prompt)-> LLM1 -> Summary1 + chunk2 -> prompt2 -> LLM2 -> summary3 + chunks 3 + .... -> last summary\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'bitcoin.pdf', 'page': 0}, page_content=\"Bitcoin: A Peer-to-Peer Electronic Cash System\\nSatoshi Nakamoto\\nsatoshin@gmx.com\\nwww.bitcoin.org\\nAbstract.  A purely peer-to-peer version of  electronic cash would allow online  \\npayments to be sent directly from one party to another without going through a  \\nfinancial institution.  Digital signatures provide part of the solution, but the main  \\nbenefits are lost if a trusted third party is still required to prevent double-spending.  \\nWe propose a solution to the double-spending problem using a peer-to-peer network. \\nThe network timestamps transactions by hashing them into an ongoing chain of  \\nhash-based proof-of-work, forming a record that cannot be changed without redoing  \\nthe proof-of-work.  The longest chain not only serves as proof of the sequence of  \\nevents witnessed, but proof that it came from the largest pool of CPU power.  As  \\nlong as a majority of CPU power is controlled by nodes that are not cooperating to  \\nattack the network, they'll generate the longest chain and outpace attackers.  The  \\nnetwork itself requires minimal structure.  Messages are broadcast on a best effort  \\nbasis, and nodes can leave and rejoin the network at will, accepting the longest  \\nproof-of-work chain as proof of what happened while they were gone.\\n1. Introduction\\nCommerce on the Internet has come to rely almost exclusively on financial institutions serving as  \\ntrusted third parties to process electronic payments.  While the system works well enough for  \\nmost  transactions,  it  still  suffers  from  the  inherent  weaknesses  of  the  trust  based  model.  \\nCompletely non-reversible transactions are not really possible, since financial institutions cannot  \\navoid  mediating  disputes.   The  cost  of  mediation  increases  transaction  costs,  limiting  the  \\nminimum practical transaction size and cutting off the possibility for small casual transactions,  \\nand there is a broader cost in the loss of ability to make non-reversible payments for non-\\nreversible services.  With the possibility of reversal, the need for trust spreads.  Merchants must  \\nbe wary of their customers, hassling them for more information than they would otherwise need.  \\nA certain percentage of fraud is accepted as unavoidable.  These costs and payment uncertainties  \\ncan be avoided in person by using physical currency, but no mechanism exists to make payments  \\nover a communications channel without a trusted party.\\nWhat is needed is an electronic payment system based on cryptographic proof instead of trust,  \\nallowing any two willing parties to transact directly with each other without the need for a trusted  \\nthird party.  Transactions that are computationally impractical to reverse would protect sellers  \\nfrom fraud, and routine escrow mechanisms could easily be implemented to protect buyers.  In  \\nthis paper, we propose a solution to the double-spending problem using a peer-to-peer distributed  \\ntimestamp server to generate computational proof of the chronological order of transactions.  The  \\nsystem  is  secure  as  long  as  honest  nodes  collectively  control  more  CPU  power  than  any  \\ncooperating group of attacker nodes.\\n1\"),\n",
       " Document(metadata={'source': 'bitcoin.pdf', 'page': 1}, page_content=\"2. Transactions\\nWe define an electronic coin as a chain of digital signatures.  Each owner transfers the coin to the  \\nnext by digitally signing a hash of the previous transaction and the public key of the next owner  \\nand adding these to the end of the coin.  A payee can verify the signatures to verify the chain of  \\nownership.\\nThe problem of course is the payee can't verify that one of the owners did not double-spend  \\nthe coin.  A common solution is to introduce a trusted central authority, or mint, that checks every \\ntransaction for double spending.  After each transaction, the coin must be returned to the mint to  \\nissue a new coin, and only coins issued directly from the mint are trusted not to be double-spent.  \\nThe  problem with  this solution  is that  the  fate of the entire  money system depends on the  \\ncompany running the mint, with every transaction having to go through them, just like a bank.\\nWe need a way for the payee to know that the previous owners did not sign any earlier  \\ntransactions.  For our purposes, the earliest transaction is the one that counts, so we don't care  \\nabout later attempts to double-spend.  The only way to confirm the absence of a transaction is to  \\nbe aware of all transactions.  In the mint based model, the mint was aware of all transactions and  \\ndecided which arrived first.  To accomplish this without a trusted party, transactions must be  \\npublicly announced [1], and we need a system for participants to agree on a single history of the  \\norder in which they were received.  The payee needs proof that at the time of each transaction, the \\nmajority of nodes agreed it was the first received. \\n3. Timestamp Server\\nThe solution we propose begins with a timestamp server.  A timestamp server works by taking a  \\nhash  of a  block  of  items  to  be  timestamped  and  widely  publishing  the  hash,  such  as  in  a  \\nnewspaper or Usenet post [2-5].  The timestamp proves that the data must have existed at the  \\ntime, obviously, in order to get into the hash.  Each timestamp includes the previous timestamp in  \\nits hash, forming a chain, with each additional timestamp reinforcing the ones before it.\\n2\\nBlock\\nItem Item ...\\nHash\\nBlock\\nItem Item ...\\nHash\\nTransaction\\nOwner 1's\\nPublic Key\\nOwner 0's\\nSignature\\nHash\\nTransaction\\nOwner 2's\\nPublic Key\\nOwner 1's\\nSignature\\nHash\\n Verify\\nTransaction\\nOwner 3's\\nPublic Key\\nOwner 2's\\nSignature\\nHash\\n Verify\\nOwner 2's\\nPrivate Key\\nOwner 1's\\nPrivate Key\\nSign  Sign  \\nOwner 3's\\nPrivate Key\"),\n",
       " Document(metadata={'source': 'bitcoin.pdf', 'page': 2}, page_content=\"4. Proof-of-Work\\nTo implement a distributed timestamp server on a peer-to-peer basis, we will need to use a proof-\\nof-work system similar to Adam Back's Hashcash [6], rather than newspaper or Usenet posts.  \\nThe proof-of-work involves scanning for a value that when hashed, such as with SHA-256, the  \\nhash begins with a number of zero bits.  The average work required is exponential in the number  \\nof zero bits required and can be verified by executing a single hash.\\nFor our timestamp network, we implement the proof-of-work by incrementing a nonce in the  \\nblock until a value is found that gives the block's hash the required zero bits.  Once the CPU  \\neffort has been expended to make it satisfy the proof-of-work, the block cannot be changed  \\nwithout redoing the work.  As later blocks are chained after it, the work to change the block  \\nwould include redoing all the blocks after it.\\nThe proof-of-work also solves the problem of determining representation in majority decision  \\nmaking.  If the majority were based on one-IP-address-one-vote, it could be subverted by anyone  \\nable  to  allocate  many  IPs.   Proof-of-work  is  essentially  one-CPU-one-vote.   The  majority  \\ndecision is represented by the longest chain, which has the greatest proof-of-work effort invested  \\nin it.  If a majority of CPU power is controlled by honest nodes, the honest chain will grow the  \\nfastest and outpace any competing chains.  To modify a past block, an attacker would have to  \\nredo the proof-of-work of the block and all blocks after it and then catch up with and surpass the  \\nwork of the honest nodes.  We will show later that the probability of a slower attacker catching up \\ndiminishes exponentially as subsequent blocks are added.\\nTo compensate for increasing hardware speed and varying interest in running nodes over time, \\nthe proof-of-work difficulty is determined by a moving average targeting an average number of  \\nblocks per hour.  If they're generated too fast, the difficulty increases.\\n5. Network\\nThe steps to run the network are as follows:\\n1) New transactions are broadcast to all nodes.\\n2) Each node collects new transactions into a block.  \\n3) Each node works on finding a difficult proof-of-work for its block.\\n4) When a node finds a proof-of-work, it broadcasts the block to all nodes.\\n5) Nodes accept the block only if all transactions in it are valid and not already spent.\\n6) Nodes express their acceptance of the block by working on creating the next block in the  \\nchain, using the hash of the accepted block as the previous hash.\\nNodes always consider the longest chain to be the correct one and will keep working on  \\nextending it.  If two nodes broadcast different versions of the next block simultaneously, some  \\nnodes may receive one or the other first.  In that case, they work on the first one they received,  \\nbut save the other branch in case it becomes longer.  The tie will be broken when the next proof-\\nof-work is found and one branch becomes longer; the nodes that were working on the other  \\nbranch will then switch to the longer one.\\n3\\nBlock\\nPrev Hash Nonce\\nTx Tx ...\\nBlock\\nPrev Hash Nonce\\nTx Tx ...\"),\n",
       " Document(metadata={'source': 'bitcoin.pdf', 'page': 3}, page_content=\"New transaction broadcasts do not necessarily need to reach all nodes.  As long as they reach  \\nmany nodes, they will get into a block before long.  Block broadcasts are also tolerant of dropped  \\nmessages.  If a node does not receive a block, it will request it when it receives the next block and \\nrealizes it missed one.\\n6. Incentive\\nBy convention, the first transaction in a block is a special transaction that starts a new coin owned \\nby the creator of the block.  This adds an incentive for nodes to support the network, and provides \\na way to initially distribute coins into circulation, since there is no central authority to issue them.  \\nThe steady addition of a constant of amount of new coins is analogous to gold miners expending  \\nresources to add gold to circulation.  In our case, it is CPU time and electricity that is expended.\\nThe incentive can also be funded with transaction fees.  If the output value of a transaction is  \\nless than its input value, the difference is a transaction fee that is added to the incentive value of  \\nthe  block  containing  the  transaction.   Once  a  predetermined  number  of  coins  have  entered  \\ncirculation, the incentive can transition entirely to transaction fees and be completely inflation  \\nfree.\\nThe incentive may help encourage nodes to stay honest.  If a greedy attacker is able to  \\nassemble more CPU power than all the honest nodes, he would have to choose between using it  \\nto defraud people by stealing back his payments, or using it to generate new coins.  He ought to  \\nfind it more profitable to play by the rules, such rules that favour him with more new coins than  \\neveryone else combined, than to undermine the system and the validity of his own wealth.\\n7. Reclaiming Disk Space\\nOnce the latest transaction in a coin is buried under enough blocks, the spent transactions before  \\nit can be discarded to save disk space.  To facilitate this without breaking the block's hash,  \\ntransactions are hashed in a Merkle Tree [7][2][5], with only the root included in the block's hash. \\nOld blocks can then be compacted by stubbing off branches of the tree.  The interior hashes do  \\nnot need to be stored.\\nA block header with no transactions would be about 80 bytes.  If we suppose blocks are  \\ngenerated every 10 minutes, 80 bytes * 6 * 24 * 365 = 4.2MB per year.  With computer systems  \\ntypically selling with 2GB of RAM as of 2008, and Moore's Law predicting current growth of  \\n1.2GB per year, storage should not be a problem even if the block headers must be kept in  \\nmemory.\\n4\\nBlockBlock\\nBlock Header (Block Hash)\\nPrev Hash Nonce\\nHash01\\nHash0 Hash1 Hash2 Hash3\\nHash23\\nRoot Hash\\nHash01\\nHash2\\nTx3\\nHash23\\nBlock Header (Block Hash)\\nRoot Hash\\nTransactions Hashed in a Merkle Tree After Pruning Tx0-2 from the Block\\nPrev Hash Nonce\\nHash3\\nTx0 Tx1 Tx2 Tx3\"),\n",
       " Document(metadata={'source': 'bitcoin.pdf', 'page': 4}, page_content=\"8. Simplified Payment Verification\\nIt is possible to verify payments without running a full network node.  A user only needs to keep  \\na copy of the block headers of the longest proof-of-work chain, which he can get by querying  \\nnetwork nodes until he's convinced he has the longest chain, and obtain the Merkle branch  \\nlinking the transaction to the block it's timestamped in.  He can't  check the transaction for  \\nhimself, but by linking it to a place in the chain, he can see that a network node has accepted it,  \\nand blocks added after it further confirm the network has accepted it.\\nAs such, the verification is reliable as long as honest nodes control the network, but is more  \\nvulnerable  if  the  network  is overpowered  by an  attacker.  While  network  nodes can  verify  \\ntransactions for themselves, the  simplified  method  can be  fooled by an attacker's fabricated  \\ntransactions for as long as the attacker can continue to overpower the network.  One strategy to  \\nprotect against this would be to accept alerts from network nodes when they detect an invalid  \\nblock, prompting  the  user's  software  to  download  the  full  block  and alerted  transactions to  \\nconfirm the inconsistency.  Businesses that receive frequent payments will probably still want to  \\nrun their own nodes for more independent security and quicker verification.\\n9. Combining and Splitting Value\\nAlthough it would be possible to handle coins individually, it would be unwieldy to make a  \\nseparate transaction for every cent in a transfer.  To allow value to be split and combined,  \\ntransactions contain multiple inputs and outputs.  Normally there will be either a single input  \\nfrom a larger previous transaction or multiple inputs combining smaller amounts, and at most two \\noutputs: one for the payment, and one returning the change, if any, back to the sender.  \\nIt should be noted that fan-out, where a transaction depends on several transactions, and those  \\ntransactions depend on many more, is not a problem here.  There is never the need to extract a  \\ncomplete standalone copy of a transaction's history.\\n5\\nTransaction\\nIn\\n...\\nIn Out\\n...\\nHash01\\nHash2 Hash3\\nHash23\\nBlock Header\\nMerkle Root\\nPrev Hash Nonce\\nBlock Header\\nMerkle Root\\nPrev Hash Nonce\\nBlock Header\\nMerkle Root\\nPrev Hash Nonce\\nMerkle Branch for Tx3\\nLongest Proof-of-Work Chain\\nTx3\"),\n",
       " Document(metadata={'source': 'bitcoin.pdf', 'page': 5}, page_content='10. Privacy\\nThe traditional banking model achieves a level of privacy by limiting access to information to the  \\nparties involved and the trusted third party.  The necessity to announce all transactions publicly  \\nprecludes this method, but privacy can still be maintained by breaking the flow of information in  \\nanother place: by keeping public keys anonymous.  The public can see that someone is sending  \\nan amount to someone else, but without information linking the transaction to anyone.  This is  \\nsimilar to the level of information released by stock exchanges, where the time and size of  \\nindividual trades, the \"tape\", is made public, but without telling who the parties were.\\nAs an additional firewall, a new key pair should be used for each transaction to keep them  \\nfrom being linked to a common owner.  Some  linking is still unavoidable with multi-input  \\ntransactions, which necessarily reveal that their inputs were owned by the same owner.  The risk  \\nis that if the owner of a key is revealed, linking could reveal other transactions that belonged to  \\nthe same owner.\\n11. Calculations\\nWe consider the scenario of an attacker trying to generate an alternate chain faster than the honest \\nchain.  Even if this is accomplished, it does not throw the system open to arbitrary changes, such  \\nas creating value out of thin air or taking money that never belonged to the attacker.  Nodes are  \\nnot going to accept an invalid transaction as payment, and honest nodes will never accept a block  \\ncontaining them.  An attacker can only try to change one of his own transactions to take back  \\nmoney he recently spent.\\nThe race between the honest chain and an attacker chain can be characterized as a Binomial  \\nRandom Walk.  The success event is the honest chain being extended by one block, increasing its  \\nlead by +1, and the failure event is the attacker\\'s chain being extended by one block, reducing the  \\ngap by -1.\\nThe probability of an attacker catching up from a given deficit is analogous to a Gambler\\'s  \\nRuin problem.  Suppose a gambler with unlimited credit starts at a deficit and plays potentially an \\ninfinite number of trials to try to reach breakeven.  We can calculate the probability he ever  \\nreaches breakeven, or that an attacker ever catches up with the honest chain, as follows [8]:\\np = probability an honest node finds the next block\\nq = probability the attacker finds the next block\\nqz = probability the attacker will ever catch up from z blocks behind\\nqz={\\n1 if p≤q\\n\\ue09eq/ p\\ue09fz\\nif p\\ue085q}\\n6\\nIdentities Transactions Trusted\\nThird Party Counterparty Public\\nIdentities Transactions Public\\nNew Privacy Model\\nTraditional Privacy Model'),\n",
       " Document(metadata={'source': 'bitcoin.pdf', 'page': 6}, page_content=\"Given our assumption that p > q, the probability drops exponentially as the number of blocks the  \\nattacker has to catch up with increases.  With the odds against him, if he doesn't make a lucky  \\nlunge forward early on, his chances become vanishingly small as he falls further behind.\\nWe now consider how long the recipient of a new transaction needs to wait before being  \\nsufficiently certain the sender can't change the transaction.  We assume the sender is an attacker  \\nwho wants to make the recipient believe he paid him for a while, then switch it to pay back to  \\nhimself after some time has passed.  The receiver will be alerted when that happens, but the  \\nsender hopes it will be too late.\\nThe receiver generates a new key pair and gives the public key to the sender shortly before  \\nsigning.  This prevents the sender from preparing a chain of blocks ahead of time by working on  \\nit continuously until he is lucky enough to get far enough ahead, then executing the transaction at  \\nthat moment.  Once the transaction is sent, the dishonest sender starts working in secret on a  \\nparallel chain containing an alternate version of his transaction.\\nThe recipient waits until the transaction has been added to a block and  z blocks have been  \\nlinked  after  it.   He  doesn't  know  the  exact  amount  of  progress  the  attacker  has  made, but  \\nassuming the honest blocks took the average expected time per block, the attacker's potential  \\nprogress will be a Poisson distribution with expected value:\\n\\ue0c1=z q\\np\\nTo get the probability the attacker could still catch up now, we multiply the Poisson density for  \\neach amount of progress he could have made by the probability he could catch up from that point:\\n∑k=0\\n∞\\n\\ue0c1k\\ne−\\ue0c1\\nk! ⋅{\\n\\ue09eq/ p\\ue09f\\ue09e z−k \\ue09f\\nif k≤ z\\n1 if k\\ue085 z}\\nRearranging to avoid summing the infinite tail of the distribution...\\n1−∑k=0\\nz\\n\\ue0c1k\\ne−\\ue0c1\\nk! \\ue09e1−\\ue09eq/ p\\ue09f\\ue09e z−k\\ue09f\\n\\ue09f\\nConverting to C code...\\n#include <math.h>\\ndouble AttackerSuccessProbability(double q, int z)\\n{\\n    double p = 1.0 - q;\\n    double lambda = z * (q / p);\\n    double sum = 1.0;\\n    int i, k;\\n    for (k = 0; k <= z; k++)\\n    {\\n        double poisson = exp(-lambda);\\n        for (i = 1; i <= k; i++)\\n            poisson *= lambda / i;\\n        sum -= poisson * (1 - pow(q / p, z - k));\\n    }\\n    return sum;\\n}\\n7\"),\n",
       " Document(metadata={'source': 'bitcoin.pdf', 'page': 7}, page_content='Running some results, we can see the probability drop off exponentially with z.\\nq=0.1\\nz=0    P=1.0000000\\nz=1    P=0.2045873\\nz=2    P=0.0509779\\nz=3    P=0.0131722\\nz=4    P=0.0034552\\nz=5    P=0.0009137\\nz=6    P=0.0002428\\nz=7    P=0.0000647\\nz=8    P=0.0000173\\nz=9    P=0.0000046\\nz=10   P=0.0000012\\nq=0.3\\nz=0    P=1.0000000\\nz=5    P=0.1773523\\nz=10   P=0.0416605\\nz=15   P=0.0101008\\nz=20   P=0.0024804\\nz=25   P=0.0006132\\nz=30   P=0.0001522\\nz=35   P=0.0000379\\nz=40   P=0.0000095\\nz=45   P=0.0000024\\nz=50   P=0.0000006\\nSolving for P less than 0.1%...\\nP < 0.001\\nq=0.10   z=5\\nq=0.15   z=8\\nq=0.20   z=11\\nq=0.25   z=15\\nq=0.30   z=24\\nq=0.35   z=41\\nq=0.40   z=89\\nq=0.45   z=340\\n12. Conclusion\\nWe have proposed a system for electronic transactions without relying on trust.  We started with  \\nthe usual framework of coins made from digital signatures, which provides strong control of  \\nownership, but is incomplete without a way to prevent double-spending.  To solve this, we  \\nproposed a peer-to-peer network using proof-of-work to record a public history of transactions  \\nthat quickly becomes computationally impractical  for  an attacker to change if honest nodes  \\ncontrol a majority of CPU power.  The network is robust in its unstructured simplicity.  Nodes  \\nwork all at once with little coordination.  They do not need to be identified, since messages are  \\nnot routed to any particular place and only need to be delivered on a best effort basis.  Nodes can  \\nleave  and  rejoin  the  network  at  will,  accepting  the  proof-of-work  chain  as  proof  of  what  \\nhappened while they were gone.  They vote with their CPU power, expressing their acceptance of  \\nvalid blocks by working on extending them and rejecting invalid blocks by refusing to work on  \\nthem.  Any needed rules and incentives can be enforced with this consensus mechanism.\\n8'),\n",
       " Document(metadata={'source': 'bitcoin.pdf', 'page': 8}, page_content='References\\n[1] W. Dai, \"b-money,\" http://www.weidai.com/bmoney.txt, 1998.\\n[2] H. Massias, X.S. Avila, and J.-J. Quisquater, \"Design of a secure timestamping service with minimal \\ntrust requirements,\" In 20th Symposium on Information Theory in the Benelux, May 1999.\\n[3] S. Haber, W.S. Stornetta, \"How to time-stamp a digital document,\" In Journal of Cryptology, vol 3, no \\n2, pages 99-111, 1991.\\n[4] D. Bayer, S. Haber, W.S. Stornetta, \"Improving the efficiency and reliability of digital time-stamping,\" \\nIn Sequences II: Methods in Communication, Security and Computer Science, pages 329-334, 1993.\\n[5] S. Haber, W.S. Stornetta, \"Secure names for bit-strings,\" In Proceedings of the 4th ACM Conference \\non Computer and Communications Security, pages 28-35, April 1997.\\n[6] A. Back, \"Hashcash - a denial of service counter-measure,\" \\nhttp://www.hashcash.org/papers/hashcash.pdf, 2002.\\n[7] R.C. Merkle, \"Protocols for public key cryptosystems,\" In Proc. 1980 Symposium on Security and \\nPrivacy, IEEE Computer Society, pages 122-133, April 1980.\\n[8] W. Feller, \"An introduction to probability theory and its applications,\" 1957.\\n9')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Stuff document chain text summarization\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf = PyPDFLoader(\"bitcoin.pdf\")\n",
    "pages = pdf.load_and_split()\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "template = \"\"\"\n",
    "write a concise and short summary of the following research paper\n",
    "Research paper : {text}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = [\"text\"],\n",
    "    template = template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For combining the prompt and model into a chain for summarization\n",
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise and short summary of the following research paper\n",
      "Research paper : Bitcoin: A Peer-to-Peer Electronic Cash System\n",
      "Satoshi Nakamoto\n",
      "satoshin@gmx.com\n",
      "www.bitcoin.org\n",
      "Abstract.  A purely peer-to-peer version of  electronic cash would allow online  \n",
      "payments to be sent directly from one party to another without going through a  \n",
      "financial institution.  Digital signatures provide part of the solution, but the main  \n",
      "benefits are lost if a trusted third party is still required to prevent double-spending.  \n",
      "We propose a solution to the double-spending problem using a peer-to-peer network. \n",
      "The network timestamps transactions by hashing them into an ongoing chain of  \n",
      "hash-based proof-of-work, forming a record that cannot be changed without redoing  \n",
      "the proof-of-work.  The longest chain not only serves as proof of the sequence of  \n",
      "events witnessed, but proof that it came from the largest pool of CPU power.  As  \n",
      "long as a majority of CPU power is controlled by nodes that are not cooperating to  \n",
      "attack the network, they'll generate the longest chain and outpace attackers.  The  \n",
      "network itself requires minimal structure.  Messages are broadcast on a best effort  \n",
      "basis, and nodes can leave and rejoin the network at will, accepting the longest  \n",
      "proof-of-work chain as proof of what happened while they were gone.\n",
      "1. Introduction\n",
      "Commerce on the Internet has come to rely almost exclusively on financial institutions serving as  \n",
      "trusted third parties to process electronic payments.  While the system works well enough for  \n",
      "most  transactions,  it  still  suffers  from  the  inherent  weaknesses  of  the  trust  based  model.  \n",
      "Completely non-reversible transactions are not really possible, since financial institutions cannot  \n",
      "avoid  mediating  disputes.   The  cost  of  mediation  increases  transaction  costs,  limiting  the  \n",
      "minimum practical transaction size and cutting off the possibility for small casual transactions,  \n",
      "and there is a broader cost in the loss of ability to make non-reversible payments for non-\n",
      "reversible services.  With the possibility of reversal, the need for trust spreads.  Merchants must  \n",
      "be wary of their customers, hassling them for more information than they would otherwise need.  \n",
      "A certain percentage of fraud is accepted as unavoidable.  These costs and payment uncertainties  \n",
      "can be avoided in person by using physical currency, but no mechanism exists to make payments  \n",
      "over a communications channel without a trusted party.\n",
      "What is needed is an electronic payment system based on cryptographic proof instead of trust,  \n",
      "allowing any two willing parties to transact directly with each other without the need for a trusted  \n",
      "third party.  Transactions that are computationally impractical to reverse would protect sellers  \n",
      "from fraud, and routine escrow mechanisms could easily be implemented to protect buyers.  In  \n",
      "this paper, we propose a solution to the double-spending problem using a peer-to-peer distributed  \n",
      "timestamp server to generate computational proof of the chronological order of transactions.  The  \n",
      "system  is  secure  as  long  as  honest  nodes  collectively  control  more  CPU  power  than  any  \n",
      "cooperating group of attacker nodes.\n",
      "1\n",
      "\n",
      "2. Transactions\n",
      "We define an electronic coin as a chain of digital signatures.  Each owner transfers the coin to the  \n",
      "next by digitally signing a hash of the previous transaction and the public key of the next owner  \n",
      "and adding these to the end of the coin.  A payee can verify the signatures to verify the chain of  \n",
      "ownership.\n",
      "The problem of course is the payee can't verify that one of the owners did not double-spend  \n",
      "the coin.  A common solution is to introduce a trusted central authority, or mint, that checks every \n",
      "transaction for double spending.  After each transaction, the coin must be returned to the mint to  \n",
      "issue a new coin, and only coins issued directly from the mint are trusted not to be double-spent.  \n",
      "The  problem with  this solution  is that  the  fate of the entire  money system depends on the  \n",
      "company running the mint, with every transaction having to go through them, just like a bank.\n",
      "We need a way for the payee to know that the previous owners did not sign any earlier  \n",
      "transactions.  For our purposes, the earliest transaction is the one that counts, so we don't care  \n",
      "about later attempts to double-spend.  The only way to confirm the absence of a transaction is to  \n",
      "be aware of all transactions.  In the mint based model, the mint was aware of all transactions and  \n",
      "decided which arrived first.  To accomplish this without a trusted party, transactions must be  \n",
      "publicly announced [1], and we need a system for participants to agree on a single history of the  \n",
      "order in which they were received.  The payee needs proof that at the time of each transaction, the \n",
      "majority of nodes agreed it was the first received. \n",
      "3. Timestamp Server\n",
      "The solution we propose begins with a timestamp server.  A timestamp server works by taking a  \n",
      "hash  of a  block  of  items  to  be  timestamped  and  widely  publishing  the  hash,  such  as  in  a  \n",
      "newspaper or Usenet post [2-5].  The timestamp proves that the data must have existed at the  \n",
      "time, obviously, in order to get into the hash.  Each timestamp includes the previous timestamp in  \n",
      "its hash, forming a chain, with each additional timestamp reinforcing the ones before it.\n",
      "2\n",
      "Block\n",
      "Item Item ...\n",
      "Hash\n",
      "Block\n",
      "Item Item ...\n",
      "Hash\n",
      "Transaction\n",
      "Owner 1's\n",
      "Public Key\n",
      "Owner 0's\n",
      "Signature\n",
      "Hash\n",
      "Transaction\n",
      "Owner 2's\n",
      "Public Key\n",
      "Owner 1's\n",
      "Signature\n",
      "Hash\n",
      " Verify\n",
      "Transaction\n",
      "Owner 3's\n",
      "Public Key\n",
      "Owner 2's\n",
      "Signature\n",
      "Hash\n",
      " Verify\n",
      "Owner 2's\n",
      "Private Key\n",
      "Owner 1's\n",
      "Private Key\n",
      "Sign  Sign  \n",
      "Owner 3's\n",
      "Private Key\n",
      "\n",
      "4. Proof-of-Work\n",
      "To implement a distributed timestamp server on a peer-to-peer basis, we will need to use a proof-\n",
      "of-work system similar to Adam Back's Hashcash [6], rather than newspaper or Usenet posts.  \n",
      "The proof-of-work involves scanning for a value that when hashed, such as with SHA-256, the  \n",
      "hash begins with a number of zero bits.  The average work required is exponential in the number  \n",
      "of zero bits required and can be verified by executing a single hash.\n",
      "For our timestamp network, we implement the proof-of-work by incrementing a nonce in the  \n",
      "block until a value is found that gives the block's hash the required zero bits.  Once the CPU  \n",
      "effort has been expended to make it satisfy the proof-of-work, the block cannot be changed  \n",
      "without redoing the work.  As later blocks are chained after it, the work to change the block  \n",
      "would include redoing all the blocks after it.\n",
      "The proof-of-work also solves the problem of determining representation in majority decision  \n",
      "making.  If the majority were based on one-IP-address-one-vote, it could be subverted by anyone  \n",
      "able  to  allocate  many  IPs.   Proof-of-work  is  essentially  one-CPU-one-vote.   The  majority  \n",
      "decision is represented by the longest chain, which has the greatest proof-of-work effort invested  \n",
      "in it.  If a majority of CPU power is controlled by honest nodes, the honest chain will grow the  \n",
      "fastest and outpace any competing chains.  To modify a past block, an attacker would have to  \n",
      "redo the proof-of-work of the block and all blocks after it and then catch up with and surpass the  \n",
      "work of the honest nodes.  We will show later that the probability of a slower attacker catching up \n",
      "diminishes exponentially as subsequent blocks are added.\n",
      "To compensate for increasing hardware speed and varying interest in running nodes over time, \n",
      "the proof-of-work difficulty is determined by a moving average targeting an average number of  \n",
      "blocks per hour.  If they're generated too fast, the difficulty increases.\n",
      "5. Network\n",
      "The steps to run the network are as follows:\n",
      "1) New transactions are broadcast to all nodes.\n",
      "2) Each node collects new transactions into a block.  \n",
      "3) Each node works on finding a difficult proof-of-work for its block.\n",
      "4) When a node finds a proof-of-work, it broadcasts the block to all nodes.\n",
      "5) Nodes accept the block only if all transactions in it are valid and not already spent.\n",
      "6) Nodes express their acceptance of the block by working on creating the next block in the  \n",
      "chain, using the hash of the accepted block as the previous hash.\n",
      "Nodes always consider the longest chain to be the correct one and will keep working on  \n",
      "extending it.  If two nodes broadcast different versions of the next block simultaneously, some  \n",
      "nodes may receive one or the other first.  In that case, they work on the first one they received,  \n",
      "but save the other branch in case it becomes longer.  The tie will be broken when the next proof-\n",
      "of-work is found and one branch becomes longer; the nodes that were working on the other  \n",
      "branch will then switch to the longer one.\n",
      "3\n",
      "Block\n",
      "Prev Hash Nonce\n",
      "Tx Tx ...\n",
      "Block\n",
      "Prev Hash Nonce\n",
      "Tx Tx ...\n",
      "\n",
      "New transaction broadcasts do not necessarily need to reach all nodes.  As long as they reach  \n",
      "many nodes, they will get into a block before long.  Block broadcasts are also tolerant of dropped  \n",
      "messages.  If a node does not receive a block, it will request it when it receives the next block and \n",
      "realizes it missed one.\n",
      "6. Incentive\n",
      "By convention, the first transaction in a block is a special transaction that starts a new coin owned \n",
      "by the creator of the block.  This adds an incentive for nodes to support the network, and provides \n",
      "a way to initially distribute coins into circulation, since there is no central authority to issue them.  \n",
      "The steady addition of a constant of amount of new coins is analogous to gold miners expending  \n",
      "resources to add gold to circulation.  In our case, it is CPU time and electricity that is expended.\n",
      "The incentive can also be funded with transaction fees.  If the output value of a transaction is  \n",
      "less than its input value, the difference is a transaction fee that is added to the incentive value of  \n",
      "the  block  containing  the  transaction.   Once  a  predetermined  number  of  coins  have  entered  \n",
      "circulation, the incentive can transition entirely to transaction fees and be completely inflation  \n",
      "free.\n",
      "The incentive may help encourage nodes to stay honest.  If a greedy attacker is able to  \n",
      "assemble more CPU power than all the honest nodes, he would have to choose between using it  \n",
      "to defraud people by stealing back his payments, or using it to generate new coins.  He ought to  \n",
      "find it more profitable to play by the rules, such rules that favour him with more new coins than  \n",
      "everyone else combined, than to undermine the system and the validity of his own wealth.\n",
      "7. Reclaiming Disk Space\n",
      "Once the latest transaction in a coin is buried under enough blocks, the spent transactions before  \n",
      "it can be discarded to save disk space.  To facilitate this without breaking the block's hash,  \n",
      "transactions are hashed in a Merkle Tree [7][2][5], with only the root included in the block's hash. \n",
      "Old blocks can then be compacted by stubbing off branches of the tree.  The interior hashes do  \n",
      "not need to be stored.\n",
      "A block header with no transactions would be about 80 bytes.  If we suppose blocks are  \n",
      "generated every 10 minutes, 80 bytes * 6 * 24 * 365 = 4.2MB per year.  With computer systems  \n",
      "typically selling with 2GB of RAM as of 2008, and Moore's Law predicting current growth of  \n",
      "1.2GB per year, storage should not be a problem even if the block headers must be kept in  \n",
      "memory.\n",
      "4\n",
      "BlockBlock\n",
      "Block Header (Block Hash)\n",
      "Prev Hash Nonce\n",
      "Hash01\n",
      "Hash0 Hash1 Hash2 Hash3\n",
      "Hash23\n",
      "Root Hash\n",
      "Hash01\n",
      "Hash2\n",
      "Tx3\n",
      "Hash23\n",
      "Block Header (Block Hash)\n",
      "Root Hash\n",
      "Transactions Hashed in a Merkle Tree After Pruning Tx0-2 from the Block\n",
      "Prev Hash Nonce\n",
      "Hash3\n",
      "Tx0 Tx1 Tx2 Tx3\n",
      "\n",
      "8. Simplified Payment Verification\n",
      "It is possible to verify payments without running a full network node.  A user only needs to keep  \n",
      "a copy of the block headers of the longest proof-of-work chain, which he can get by querying  \n",
      "network nodes until he's convinced he has the longest chain, and obtain the Merkle branch  \n",
      "linking the transaction to the block it's timestamped in.  He can't  check the transaction for  \n",
      "himself, but by linking it to a place in the chain, he can see that a network node has accepted it,  \n",
      "and blocks added after it further confirm the network has accepted it.\n",
      "As such, the verification is reliable as long as honest nodes control the network, but is more  \n",
      "vulnerable  if  the  network  is overpowered  by an  attacker.  While  network  nodes can  verify  \n",
      "transactions for themselves, the  simplified  method  can be  fooled by an attacker's fabricated  \n",
      "transactions for as long as the attacker can continue to overpower the network.  One strategy to  \n",
      "protect against this would be to accept alerts from network nodes when they detect an invalid  \n",
      "block, prompting  the  user's  software  to  download  the  full  block  and alerted  transactions to  \n",
      "confirm the inconsistency.  Businesses that receive frequent payments will probably still want to  \n",
      "run their own nodes for more independent security and quicker verification.\n",
      "9. Combining and Splitting Value\n",
      "Although it would be possible to handle coins individually, it would be unwieldy to make a  \n",
      "separate transaction for every cent in a transfer.  To allow value to be split and combined,  \n",
      "transactions contain multiple inputs and outputs.  Normally there will be either a single input  \n",
      "from a larger previous transaction or multiple inputs combining smaller amounts, and at most two \n",
      "outputs: one for the payment, and one returning the change, if any, back to the sender.  \n",
      "It should be noted that fan-out, where a transaction depends on several transactions, and those  \n",
      "transactions depend on many more, is not a problem here.  There is never the need to extract a  \n",
      "complete standalone copy of a transaction's history.\n",
      "5\n",
      "Transaction\n",
      "In\n",
      "...\n",
      "In Out\n",
      "...\n",
      "Hash01\n",
      "Hash2 Hash3\n",
      "Hash23\n",
      "Block Header\n",
      "Merkle Root\n",
      "Prev Hash Nonce\n",
      "Block Header\n",
      "Merkle Root\n",
      "Prev Hash Nonce\n",
      "Block Header\n",
      "Merkle Root\n",
      "Prev Hash Nonce\n",
      "Merkle Branch for Tx3\n",
      "Longest Proof-of-Work Chain\n",
      "Tx3\n",
      "\n",
      "10. Privacy\n",
      "The traditional banking model achieves a level of privacy by limiting access to information to the  \n",
      "parties involved and the trusted third party.  The necessity to announce all transactions publicly  \n",
      "precludes this method, but privacy can still be maintained by breaking the flow of information in  \n",
      "another place: by keeping public keys anonymous.  The public can see that someone is sending  \n",
      "an amount to someone else, but without information linking the transaction to anyone.  This is  \n",
      "similar to the level of information released by stock exchanges, where the time and size of  \n",
      "individual trades, the \"tape\", is made public, but without telling who the parties were.\n",
      "As an additional firewall, a new key pair should be used for each transaction to keep them  \n",
      "from being linked to a common owner.  Some  linking is still unavoidable with multi-input  \n",
      "transactions, which necessarily reveal that their inputs were owned by the same owner.  The risk  \n",
      "is that if the owner of a key is revealed, linking could reveal other transactions that belonged to  \n",
      "the same owner.\n",
      "11. Calculations\n",
      "We consider the scenario of an attacker trying to generate an alternate chain faster than the honest \n",
      "chain.  Even if this is accomplished, it does not throw the system open to arbitrary changes, such  \n",
      "as creating value out of thin air or taking money that never belonged to the attacker.  Nodes are  \n",
      "not going to accept an invalid transaction as payment, and honest nodes will never accept a block  \n",
      "containing them.  An attacker can only try to change one of his own transactions to take back  \n",
      "money he recently spent.\n",
      "The race between the honest chain and an attacker chain can be characterized as a Binomial  \n",
      "Random Walk.  The success event is the honest chain being extended by one block, increasing its  \n",
      "lead by +1, and the failure event is the attacker's chain being extended by one block, reducing the  \n",
      "gap by -1.\n",
      "The probability of an attacker catching up from a given deficit is analogous to a Gambler's  \n",
      "Ruin problem.  Suppose a gambler with unlimited credit starts at a deficit and plays potentially an \n",
      "infinite number of trials to try to reach breakeven.  We can calculate the probability he ever  \n",
      "reaches breakeven, or that an attacker ever catches up with the honest chain, as follows [8]:\n",
      "p = probability an honest node finds the next block\n",
      "q = probability the attacker finds the next block\n",
      "qz = probability the attacker will ever catch up from z blocks behind\n",
      "qz={\n",
      "1 if p≤q\n",
      "q/ pz\n",
      "if pq}\n",
      "6\n",
      "Identities Transactions Trusted\n",
      "Third Party Counterparty Public\n",
      "Identities Transactions Public\n",
      "New Privacy Model\n",
      "Traditional Privacy Model\n",
      "\n",
      "Given our assumption that p > q, the probability drops exponentially as the number of blocks the  \n",
      "attacker has to catch up with increases.  With the odds against him, if he doesn't make a lucky  \n",
      "lunge forward early on, his chances become vanishingly small as he falls further behind.\n",
      "We now consider how long the recipient of a new transaction needs to wait before being  \n",
      "sufficiently certain the sender can't change the transaction.  We assume the sender is an attacker  \n",
      "who wants to make the recipient believe he paid him for a while, then switch it to pay back to  \n",
      "himself after some time has passed.  The receiver will be alerted when that happens, but the  \n",
      "sender hopes it will be too late.\n",
      "The receiver generates a new key pair and gives the public key to the sender shortly before  \n",
      "signing.  This prevents the sender from preparing a chain of blocks ahead of time by working on  \n",
      "it continuously until he is lucky enough to get far enough ahead, then executing the transaction at  \n",
      "that moment.  Once the transaction is sent, the dishonest sender starts working in secret on a  \n",
      "parallel chain containing an alternate version of his transaction.\n",
      "The recipient waits until the transaction has been added to a block and  z blocks have been  \n",
      "linked  after  it.   He  doesn't  know  the  exact  amount  of  progress  the  attacker  has  made, but  \n",
      "assuming the honest blocks took the average expected time per block, the attacker's potential  \n",
      "progress will be a Poisson distribution with expected value:\n",
      "=z q\n",
      "p\n",
      "To get the probability the attacker could still catch up now, we multiply the Poisson density for  \n",
      "each amount of progress he could have made by the probability he could catch up from that point:\n",
      "∑k=0\n",
      "∞\n",
      "k\n",
      "e−\n",
      "k! ⋅{\n",
      "q/ p z−k \n",
      "if k≤ z\n",
      "1 if k z}\n",
      "Rearranging to avoid summing the infinite tail of the distribution...\n",
      "1−∑k=0\n",
      "z\n",
      "k\n",
      "e−\n",
      "k! 1−q/ p z−k\n",
      "\n",
      "Converting to C code...\n",
      "#include <math.h>\n",
      "double AttackerSuccessProbability(double q, int z)\n",
      "{\n",
      "    double p = 1.0 - q;\n",
      "    double lambda = z * (q / p);\n",
      "    double sum = 1.0;\n",
      "    int i, k;\n",
      "    for (k = 0; k <= z; k++)\n",
      "    {\n",
      "        double poisson = exp(-lambda);\n",
      "        for (i = 1; i <= k; i++)\n",
      "            poisson *= lambda / i;\n",
      "        sum -= poisson * (1 - pow(q / p, z - k));\n",
      "    }\n",
      "    return sum;\n",
      "}\n",
      "7\n",
      "\n",
      "Running some results, we can see the probability drop off exponentially with z.\n",
      "q=0.1\n",
      "z=0    P=1.0000000\n",
      "z=1    P=0.2045873\n",
      "z=2    P=0.0509779\n",
      "z=3    P=0.0131722\n",
      "z=4    P=0.0034552\n",
      "z=5    P=0.0009137\n",
      "z=6    P=0.0002428\n",
      "z=7    P=0.0000647\n",
      "z=8    P=0.0000173\n",
      "z=9    P=0.0000046\n",
      "z=10   P=0.0000012\n",
      "q=0.3\n",
      "z=0    P=1.0000000\n",
      "z=5    P=0.1773523\n",
      "z=10   P=0.0416605\n",
      "z=15   P=0.0101008\n",
      "z=20   P=0.0024804\n",
      "z=25   P=0.0006132\n",
      "z=30   P=0.0001522\n",
      "z=35   P=0.0000379\n",
      "z=40   P=0.0000095\n",
      "z=45   P=0.0000024\n",
      "z=50   P=0.0000006\n",
      "Solving for P less than 0.1%...\n",
      "P < 0.001\n",
      "q=0.10   z=5\n",
      "q=0.15   z=8\n",
      "q=0.20   z=11\n",
      "q=0.25   z=15\n",
      "q=0.30   z=24\n",
      "q=0.35   z=41\n",
      "q=0.40   z=89\n",
      "q=0.45   z=340\n",
      "12. Conclusion\n",
      "We have proposed a system for electronic transactions without relying on trust.  We started with  \n",
      "the usual framework of coins made from digital signatures, which provides strong control of  \n",
      "ownership, but is incomplete without a way to prevent double-spending.  To solve this, we  \n",
      "proposed a peer-to-peer network using proof-of-work to record a public history of transactions  \n",
      "that quickly becomes computationally impractical  for  an attacker to change if honest nodes  \n",
      "control a majority of CPU power.  The network is robust in its unstructured simplicity.  Nodes  \n",
      "work all at once with little coordination.  They do not need to be identified, since messages are  \n",
      "not routed to any particular place and only need to be delivered on a best effort basis.  Nodes can  \n",
      "leave  and  rejoin  the  network  at  will,  accepting  the  proof-of-work  chain  as  proof  of  what  \n",
      "happened while they were gone.  They vote with their CPU power, expressing their acceptance of  \n",
      "valid blocks by working on extending them and rejecting invalid blocks by refusing to work on  \n",
      "them.  Any needed rules and incentives can be enforced with this consensus mechanism.\n",
      "8\n",
      "\n",
      "References\n",
      "[1] W. Dai, \"b-money,\" http://www.weidai.com/bmoney.txt, 1998.\n",
      "[2] H. Massias, X.S. Avila, and J.-J. Quisquater, \"Design of a secure timestamping service with minimal \n",
      "trust requirements,\" In 20th Symposium on Information Theory in the Benelux, May 1999.\n",
      "[3] S. Haber, W.S. Stornetta, \"How to time-stamp a digital document,\" In Journal of Cryptology, vol 3, no \n",
      "2, pages 99-111, 1991.\n",
      "[4] D. Bayer, S. Haber, W.S. Stornetta, \"Improving the efficiency and reliability of digital time-stamping,\" \n",
      "In Sequences II: Methods in Communication, Security and Computer Science, pages 329-334, 1993.\n",
      "[5] S. Haber, W.S. Stornetta, \"Secure names for bit-strings,\" In Proceedings of the 4th ACM Conference \n",
      "on Computer and Communications Security, pages 28-35, April 1997.\n",
      "[6] A. Back, \"Hashcash - a denial of service counter-measure,\" \n",
      "http://www.hashcash.org/papers/hashcash.pdf, 2002.\n",
      "[7] R.C. Merkle, \"Protocols for public key cryptosystems,\" In Proc. 1980 Symposium on Security and \n",
      "Privacy, IEEE Computer Society, pages 122-133, April 1980.\n",
      "[8] W. Feller, \"An introduction to probability theory and its applications,\" 1957.\n",
      "9\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Here is a concise and short summary of the research paper \"Bitcoin: A Peer-to-Peer Electronic Cash System\" by Satoshi Nakamoto:\\n\\nThe paper proposes a decentralized electronic cash system that allows for peer-to-peer transactions without the need for a trusted third party. The system uses a combination of digital signatures and a proof-of-work consensus mechanism to ensure the integrity and security of transactions.\\n\\nThe system is based on a distributed ledger called the \"blockchain,\" which is maintained by a network of nodes that validate and add new transactions to the blockchain. The blockchain is designed to be tamper-proof, making it difficult for an attacker to alter or manipulate transactions.\\n\\nThe paper also introduces the concept of \"mining,\" which is the process of adding new transactions to the blockchain and verifying their validity. Miners are incentivized to do so by the possibility of earning new coins as a reward for their efforts.\\n\\nThe system is designed to be decentralized, with no central authority controlling it. Nodes in the network communicate with each other using a peer-to-peer protocol, and the blockchain is updated in real-time as new transactions are added.\\n\\nThe paper concludes that the system is robust and secure, with a low risk of double-spending or tampering with transactions. The authors also discuss the potential for the system to be used for other applications beyond electronic cash, such as smart contracts and decentralized applications.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load_summarize_chain combines all the documents and send it to \n",
    "## prompt template\n",
    "chain = load_summarize_chain(llm,\n",
    "                              chain_type = 'stuff',\n",
    "                                prompt = prompt,\n",
    "                                 verbose = True)\n",
    "output_summary = chain.run(pages)\n",
    "output_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 0}, page_content='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser ∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 0}, page_content='architectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 1}, page_content='Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 1}, page_content='the number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\\nof continuous representations z = (z1,...,z n). Given z, the decoder then generates an output\\nsequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 1}, page_content='of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output\\nsequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 2}, page_content='Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 3}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices Kand V. We compute\\nthe matrix of outputs as:\\nAttention(Q,K,V ) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 3}, page_content='queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = ∑dk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 4}, page_content='MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\\nwhere headi = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈Rdmodel×dk , WK\\ni ∈Rdmodel×dk , WV\\ni ∈Rdmodel×dv\\nand WO ∈Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 4}, page_content='connected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0,xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 5}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 ·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel )\\nPE(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto 10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 5}, page_content='(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 6}, page_content='the input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k<n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 15], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+ n·d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 6}, page_content='batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate= d−0.5\\nmodel ·min(step_num−0.5,step_num·warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup_stepstraining steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps= 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\nResidual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\n7'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 7}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [15] 23.75\\nDeep-Att + PosUnk [32] 39.2 1.0 ·1020\\nGNMT + RL [31] 24.6 39.92 2.3 ·1019 1.4 ·1020\\nConvS2S [8] 25.16 40.46 9.6 ·1018 1.5 ·1020\\nMoE [26] 26.03 40.56 2.0 ·1019 1.2 ·1020\\nDeep-Att + PosUnk Ensemble [32] 40.4 8.0 ·1020\\nGNMT + RL Ensemble [31] 26.30 41.16 1.8 ·1020 1.1 ·1021\\nConvS2S Ensemble [8] 26.36 41.29 7.7 ·1019 1.2 ·1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.0 2.3 ·1019\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [30]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 7}, page_content='For the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α= 0.6 [31]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 8}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 8}, page_content='English-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 9}, page_content='References\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 9}, page_content='9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n10'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 10}, page_content='[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang'),\n",
       " Document(metadata={'source': 'AttentionIsAllYouNeed.pdf', 'page': 10}, page_content='Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n11')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## map reduce\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "pdf = PyPDFLoader(\"AttentionIsAllYouNeed.pdf\").load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 2000,\n",
    "                                         chunk_overlap = 200)\n",
    "docs = splitter.split_documents(pdf)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each documents will be summarized separately and then finally they re combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Number of documents\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "chunks_prompt = \"\"\"\n",
    "write a concise and short summary of the following research paper\n",
    "Research paper : {text}\n",
    "\"\"\"\n",
    "map_prompt_template = PromptTemplate(\n",
    "    input_variables = [\"text\"],\n",
    "    template = chunks_prompt\n",
    ")\n",
    "final_prompt = \"\"\"\n",
    "Provide the final summary of the entire reseach paper\n",
    "with these important points. Add a suitable title and \n",
    "tell an application which uses/can use the technology \n",
    "mentioned in the research paper\n",
    "Points : {text}\n",
    "\"\"\"\n",
    "final_prompt_template = PromptTemplate(\n",
    "    input_variables = ['text'],\n",
    "    template = final_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise and short summary of the following research paper\n",
      "Research paper : Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗†\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Łukasz Kaiser ∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring signiﬁcantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.0 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature.\n",
      "1 Introduction\n",
      "Recurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\n",
      "in particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\n",
      "transduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\n",
      "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
      "architectures [31, 21, 13].\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise and short summary of the following research paper\n",
      "Research paper : architectures [31, 21, 13].\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
      "our research.\n",
      "†Work performed while at Google Brain.\n",
      "‡Work performed while at Google Research.\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise and short summary of the following research paper\n",
      "Research paper : Recurrent models typically factor computation along the symbol positions of the input and output\n",
      "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
      "states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\n",
      "sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
      "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n",
      "signiﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\n",
      "computation [26], while also improving model performance in case of the latter. The fundamental\n",
      "constraint of sequential computation, however, remains.\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\n",
      "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
      "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise and short summary of the following research paper\n",
      "Research paper : the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 22, 23, 19].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [28].\n",
      "To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [14, 15] and [8].\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\n",
      "Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\n",
      "of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output\n",
      "sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise and short summary of the following research paper\n",
      "Research paper : of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output\n",
      "sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\n",
      "[9], consuming the previously generated symbols as additional input when generating the next.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "2\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise and short summary of the following research paper\n",
      "Research paper : Figure 1: The Transformer - model architecture.\n",
      "wise fully connected feed-forward network. We employ a residual connection [10] around each of\n",
      "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
      "LayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\n",
      "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
      "layers, produce outputs of dimension dmodel = 512.\n",
      "Decoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\n",
      "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
      "attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
      "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
      "predictions for position ican depend only on the known outputs at positions less than i.\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      "query with the corresponding key.\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "3\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise and short summary of the following research paper\n",
      "Research paper : Scaled Dot-Product Attention\n",
      " Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention layers running in parallel.\n",
      "query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\n",
      "values.\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
      "into a matrix Q. The keys and values are also packed together into matrices Kand V. We compute\n",
      "the matrix of outputs as:\n",
      "Attention(Q,K,V ) = softmax(QKT\n",
      "√dk\n",
      ")V (1)\n",
      "The two most commonly used attention functions are additive attention [2], and dot-product (multi-\n",
      "plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
      "of 1√dk\n",
      ". Additive attention computes the compatibility function using a feed-forward network with\n",
      "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
      "much faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\n",
      "matrix multiplication code.\n",
      "While for small values of dk the two mechanisms perform similarly, additive attention outperforms\n",
      "dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\n",
      "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
      "extremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\n",
      ".\n",
      "3.2.2 Multi-Head Attention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
      "we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\n",
      "linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\n",
      "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise and short summary of the following research paper\n",
      "Research paper : queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
      "output values. These are concatenated and once again projected, resulting in the ﬁnal values, as\n",
      "depicted in Figure 2.\n",
      "Multi-head attention allows the model to jointly attend to information from different representation\n",
      "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "4To illustrate why the dot products get large, assume that the components of q and k are independent random\n",
      "variables with mean 0 and variance 1. Then their dot product, q · k = ∑dk\n",
      "i=1 qiki, has mean 0 and variance dk.\n",
      "4\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise and short summary of the following research paper\n",
      "Research paper : MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\n",
      "where headi = Attention(QWQ\n",
      "i ,KW K\n",
      "i ,VW V\n",
      "i )\n",
      "Where the projections are parameter matricesWQ\n",
      "i ∈Rdmodel×dk , WK\n",
      "i ∈Rdmodel×dk , WV\n",
      "i ∈Rdmodel×dv\n",
      "and WO ∈Rhdv×dmodel .\n",
      "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
      "dk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\n",
      "is similar to that of single-head attention with full dimensionality.\n",
      "3.2.3 Applications of Attention in our Model\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
      "and the memory keys and values come from the output of the encoder. This allows every\n",
      "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[31, 2, 8].\n",
      "• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
      "and queries come from the same place, in this case, the output of the previous layer in the\n",
      "encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
      "encoder.\n",
      "• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
      "all positions in the decoder up to and including that position. We need to prevent leftward\n",
      "information ﬂow in the decoder to preserve the auto-regressive property. We implement this\n",
      "inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\n",
      "of the softmax which correspond to illegal connections. See Figure 2.\n",
      "3.3 Position-wise Feed-Forward Networks\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
      "connected feed-forward network, which is applied to each position separately and identically. This\n",
      "consists of two linear transformations with a ReLU activation in between.\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise and short summary of the following research paper\n",
      "Research paper : connected feed-forward network, which is applied to each position separately and identically. This\n",
      "consists of two linear transformations with a ReLU activation in between.\n",
      "FFN(x) = max(0,xW1 + b1)W2 + b2 (2)\n",
      "While the linear transformations are the same across different positions, they use different parameters\n",
      "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
      "The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\n",
      "dff = 2048.\n",
      "3.4 Embeddings and Softmax\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
      "tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\n",
      "mation and softmax function to convert the decoder output to predicted next-token probabilities. In\n",
      "our model, we share the same weight matrix between the two embedding layers and the pre-softmax\n",
      "linear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.\n",
      "3.5 Positional Encoding\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
      "order of the sequence, we must inject some information about the relative or absolute position of the\n",
      "tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
      "5\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise and short summary of the following research paper\n",
      "Research paper : Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\n",
      "for different layer types. nis the sequence length, dis the representation dimension, kis the kernel\n",
      "size of convolutions and rthe size of the neighborhood in restricted self-attention.\n",
      "Layer Type Complexity per Layer Sequential Maximum Path Length\n",
      "Operations\n",
      "Self-Attention O(n2 ·d) O(1) O(1)\n",
      "Recurrent O(n·d2) O(n) O(n)\n",
      "Convolutional O(k·n·d2) O(1) O(logk(n))\n",
      "Self-Attention (restricted) O(r·n·d) O(1) O(n/r)\n",
      "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
      "as the embeddings, so that the two can be summed. There are many choices of positional encodings,\n",
      "learned and ﬁxed [8].\n",
      "In this work, we use sine and cosine functions of different frequencies:\n",
      "PE(pos,2i) = sin(pos/100002i/dmodel )\n",
      "PE(pos,2i+1) = cos(pos/100002i/dmodel )\n",
      "where posis the position and iis the dimension. That is, each dimension of the positional encoding\n",
      "corresponds to a sinusoid. The wavelengths form a geometric progression from 2πto 10000 ·2π. We\n",
      "chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
      "relative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of\n",
      "PEpos.\n",
      "We also experimented with using learned positional embeddings [8] instead, and found that the two\n",
      "versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\n",
      "because it may allow the model to extrapolate to sequence lengths longer than the ones encountered\n",
      "during training.\n",
      "4 Why Self-Attention\n",
      "In this section we compare various aspects of self-attention layers to the recurrent and convolu-\n",
      "tional layers commonly used for mapping one variable-length sequence of symbol representations\n",
      "(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden\n",
      "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise and short summary of the following research paper\n",
      "Research paper : (x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden\n",
      "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\n",
      "consider three desiderata.\n",
      "One is the total computational complexity per layer. Another is the amount of computation that can\n",
      "be parallelized, as measured by the minimum number of sequential operations required.\n",
      "The third is the path length between long-range dependencies in the network. Learning long-range\n",
      "dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\n",
      "ability to learn such dependencies is the length of the paths forward and backward signals have to\n",
      "traverse in the network. The shorter these paths between any combination of positions in the input\n",
      "and output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\n",
      "the maximum path length between any two input and output positions in networks composed of the\n",
      "different layer types.\n",
      "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\n",
      "executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\n",
      "computational complexity, self-attention layers are faster than recurrent layers when the sequence\n",
      "length n is smaller than the representation dimensionality d, which is most often the case with\n",
      "sentence representations used by state-of-the-art models in machine translations, such as word-piece\n",
      "[31] and byte-pair [25] representations. To improve computational performance for tasks involving\n",
      "very long sequences, self-attention could be restricted to considering only a neighborhood of size rin\n",
      "6\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise and short summary of the following research paper\n",
      "Research paper : the input sequence centered around the respective output position. This would increase the maximum\n",
      "path length to O(n/r). We plan to investigate this approach further in future work.\n",
      "A single convolutional layer with kernel width k<n does not connect all pairs of input and output\n",
      "positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\n",
      "or O(logk(n)) in the case of dilated convolutions [ 15], increasing the length of the longest paths\n",
      "between any two positions in the network. Convolutional layers are generally more expensive than\n",
      "recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\n",
      "considerably, to O(k·n·d+ n·d2). Even with k = n, however, the complexity of a separable\n",
      "convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\n",
      "the approach we take in our model.\n",
      "As side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\n",
      "from our models and present and discuss examples in the appendix. Not only do individual attention\n",
      "heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\n",
      "and semantic structure of the sentences.\n",
      "5 Training\n",
      "This section describes the training regime for our models.\n",
      "5.1 Training Data and Batching\n",
      "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
      "sentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\n",
      "target vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\n",
      "2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\n",
      "vocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\n",
      "batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\n",
      "target tokens.\n",
      "5.2 Hardware and Schedule\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise and short summary of the following research paper\n",
      "Research paper : batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\n",
      "target tokens.\n",
      "5.2 Hardware and Schedule\n",
      "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\n",
      "the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\n",
      "trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\n",
      "bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n",
      "(3.5 days).\n",
      "5.3 Optimizer\n",
      "We used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ= 10−9. We varied the learning\n",
      "rate over the course of training, according to the formula:\n",
      "lrate= d−0.5\n",
      "model ·min(step_num−0.5,step_num·warmup_steps−1.5) (3)\n",
      "This corresponds to increasing the learning rate linearly for the ﬁrst warmup_stepstraining steps,\n",
      "and decreasing it thereafter proportionally to the inverse square root of the step number. We used\n",
      "warmup_steps= 4000.\n",
      "5.4 Regularization\n",
      "We employ three types of regularization during training:\n",
      "Residual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the\n",
      "sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\n",
      "positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\n",
      "Pdrop = 0.1.\n",
      "7\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise and short summary of the following research paper\n",
      "Research paper : Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\n",
      "English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\n",
      "Model\n",
      "BLEU Training Cost (FLOPs)\n",
      "EN-DE EN-FR EN-DE EN-FR\n",
      "ByteNet [15] 23.75\n",
      "Deep-Att + PosUnk [32] 39.2 1.0 ·1020\n",
      "GNMT + RL [31] 24.6 39.92 2.3 ·1019 1.4 ·1020\n",
      "ConvS2S [8] 25.16 40.46 9.6 ·1018 1.5 ·1020\n",
      "MoE [26] 26.03 40.56 2.0 ·1019 1.2 ·1020\n",
      "Deep-Att + PosUnk Ensemble [32] 40.4 8.0 ·1020\n",
      "GNMT + RL Ensemble [31] 26.30 41.16 1.8 ·1020 1.1 ·1021\n",
      "ConvS2S Ensemble [8] 26.36 41.29 7.7 ·1019 1.2 ·1021\n",
      "Transformer (base model) 27.3 38.1 3.3 · 1018\n",
      "Transformer (big) 28.4 41.0 2.3 ·1019\n",
      "Label Smoothing During training, we employed label smoothing of value ϵls = 0.1 [30]. This\n",
      "hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "6 Results\n",
      "6.1 Machine Translation\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\n",
      "in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\n",
      "BLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\n",
      "listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\n",
      "surpasses all previously published models and ensembles, at a fraction of the training cost of any of\n",
      "the competitive models.\n",
      "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\n",
      "outperforming all of the previously published single models, at less than 1/4 the training cost of the\n",
      "previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\n",
      "dropout rate Pdrop = 0.1, instead of 0.3.\n",
      "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which\n",
      "were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise and short summary of the following research paper\n",
      "Research paper : For the base models, we used a single model obtained by averaging the last 5 checkpoints, which\n",
      "were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\n",
      "used beam search with a beam size of 4 and length penalty α= 0.6 [31]. These hyperparameters\n",
      "were chosen after experimentation on the development set. We set the maximum output length during\n",
      "inference to input length + 50, but terminate early when possible [31].\n",
      "Table 2 summarizes our results and compares our translation quality and training costs to other model\n",
      "architectures from the literature. We estimate the number of ﬂoating point operations used to train a\n",
      "model by multiplying the training time, the number of GPUs used, and an estimate of the sustained\n",
      "single-precision ﬂoating-point capacity of each GPU 5.\n",
      "6.2 Model Variations\n",
      "To evaluate the importance of different components of the Transformer, we varied our base model\n",
      "in different ways, measuring the change in performance on English-to-German translation on the\n",
      "development set, newstest2013. We used beam search as described in the previous section, but no\n",
      "checkpoint averaging. We present these results in Table 3.\n",
      "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\n",
      "keeping the amount of computation constant, as described in Section 3.2.2. While single-head\n",
      "attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
      "5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n",
      "8\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise and short summary of the following research paper\n",
      "Research paper : Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\n",
      "model. All metrics are on the English-to-German translation development set, newstest2013. Listed\n",
      "perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\n",
      "per-word perplexities.\n",
      "N d model dff h d k dv Pdrop ϵls\n",
      "train PPL BLEU params\n",
      "steps (dev) (dev) ×106\n",
      "base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n",
      "(A)\n",
      "1 512 512 5.29 24.9\n",
      "4 128 128 5.00 25.5\n",
      "16 32 32 4.91 25.8\n",
      "32 16 16 5.01 25.4\n",
      "(B) 16 5.16 25.1 58\n",
      "32 5.01 25.4 60\n",
      "(C)\n",
      "2 6.11 23.7 36\n",
      "4 5.19 25.3 50\n",
      "8 4.88 25.5 80\n",
      "256 32 32 5.75 24.5 28\n",
      "1024 128 128 4.66 26.0 168\n",
      "1024 5.12 25.4 53\n",
      "4096 4.75 26.2 90\n",
      "(D)\n",
      "0.0 5.77 24.6\n",
      "0.2 4.95 25.5\n",
      "0.0 4.67 25.3\n",
      "0.2 5.47 25.7\n",
      "(E) positional embedding instead of sinusoids 4.92 25.7\n",
      "big 6 1024 4096 16 0.3 300K 4.33 26.4 213\n",
      "In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\n",
      "suggests that determining compatibility is not easy and that a more sophisticated compatibility\n",
      "function than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\n",
      "bigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\n",
      "sinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\n",
      "results to the base model.\n",
      "7 Conclusion\n",
      "In this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\n",
      "attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\n",
      "multi-headed self-attention.\n",
      "For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\n",
      "on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\n",
      "English-to-French translation tasks, we achieve a new state of the art. In the former task our best\n",
      "model outperforms even all previously reported ensembles.\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise and short summary of the following research paper\n",
      "Research paper : English-to-French translation tasks, we achieve a new state of the art. In the former task our best\n",
      "model outperforms even all previously reported ensembles.\n",
      "We are excited about the future of attention-based models and plan to apply them to other tasks. We\n",
      "plan to extend the Transformer to problems involving input and output modalities other than text and\n",
      "to investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\n",
      "such as images, audio and video. Making generation less sequential is another research goals of ours.\n",
      "The code we used to train and evaluate our models is available at https://github.com/\n",
      "tensorflow/tensor2tensor.\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
      "comments, corrections and inspiration.\n",
      "9\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise and short summary of the following research paper\n",
      "Research paper : References\n",
      "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\n",
      "arXiv:1607.06450, 2016.\n",
      "[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n",
      "learning to align and translate. CoRR, abs/1409.0473, 2014.\n",
      "[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\n",
      "machine translation architectures. CoRR, abs/1703.03906, 2017.\n",
      "[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\n",
      "reading. arXiv preprint arXiv:1601.06733, 2016.\n",
      "[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\n",
      "and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\n",
      "machine translation. CoRR, abs/1406.1078, 2014.\n",
      "[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\n",
      "preprint arXiv:1610.02357, 2016.\n",
      "[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\n",
      "of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n",
      "[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\n",
      "tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n",
      "[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\n",
      "arXiv:1308.0850, 2013.\n",
      "[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\n",
      "age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\n",
      "Recognition, pages 770–778, 2016.\n",
      "[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\n",
      "recurrent nets: the difﬁculty of learning long-term dependencies, 2001.\n",
      "[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\n",
      "9(8):1735–1780, 1997.\n",
      "[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise and short summary of the following research paper\n",
      "Research paper : 9(8):1735–1780, 1997.\n",
      "[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\n",
      "the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n",
      "[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\n",
      "on Learning Representations (ICLR), 2016.\n",
      "[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\n",
      "ray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\n",
      "2017.\n",
      "[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\n",
      "In International Conference on Learning Representations, 2017.\n",
      "[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n",
      "[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\n",
      "arXiv:1703.10722, 2017.\n",
      "[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\n",
      "Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\n",
      "arXiv:1703.03130, 2017.\n",
      "[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\n",
      "Information Processing Systems, (NIPS), 2016.\n",
      "10\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise and short summary of the following research paper\n",
      "Research paper : [21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\n",
      "based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n",
      "[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\n",
      "model. In Empirical Methods in Natural Language Processing, 2016.\n",
      "[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\n",
      "summarization. arXiv preprint arXiv:1705.04304, 2017.\n",
      "[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\n",
      "preprint arXiv:1608.05859, 2016.\n",
      "[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\n",
      "with subword units. arXiv preprint arXiv:1508.07909, 2015.\n",
      "[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\n",
      "and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\n",
      "layer. arXiv preprint arXiv:1701.06538, 2017.\n",
      "[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\n",
      "nov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\n",
      "Learning Research, 15(1):1929–1958, 2014.\n",
      "[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\n",
      "networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\n",
      "Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\n",
      "Inc., 2015.\n",
      "[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\n",
      "networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\n",
      "[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\n",
      "Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n",
      "[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\n",
      "\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "write a concise and short summary of the following research paper\n",
      "Research paper : Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n",
      "[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\n",
      "Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\n",
      "translation system: Bridging the gap between human and machine translation. arXiv preprint\n",
      "arXiv:1609.08144, 2016.\n",
      "[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\n",
      "fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n",
      "11\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2803 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Provide the final summary of the entire reseach paper\n",
      "with these important points. Add a suitable title and \n",
      "tell an application which uses/can use the technology \n",
      "mentioned in the research paper\n",
      "Points : Here is a concise and short summary of the research paper \"Attention Is All You Need\":\n",
      "\n",
      "The paper proposes a new neural network architecture, the Transformer, that achieves state-of-the-art results in machine translation tasks without using recurrent or convolutional neural networks. Instead, the Transformer relies solely on attention mechanisms to process input sequences. The authors show that their model outperforms existing models on two machine translation tasks, requiring significantly less training time and being more parallelizable. The Transformer achieves a new state-of-the-art BLEU score of 41.0 on the WMT 2014 English-to-French translation task.\n",
      "\n",
      "Here is a concise and short summary of the research paper:\n",
      "\n",
      "This paper does not contain a research paper, but rather a list of acknowledgments and contributors to the development of the Transformer architecture. The authors acknowledge the contributions of several individuals, including Jakob, Ashish, Noam, Niki, Llion, Lukasz, and Aidan, who worked together to design, implement, and evaluate the Transformer model.\n",
      "\n",
      "Here is a concise and short summary of the research paper:\n",
      "\n",
      "The paper proposes a new model architecture called the Transformer, which replaces traditional recurrent neural networks (RNNs) with an attention mechanism to model dependencies between input and output sequences. This allows for more parallelization and significant improvements in computational efficiency, enabling the model to achieve state-of-the-art results in translation quality after only 12 hours of training on 8 GPUs.\n",
      "\n",
      "Here is a concise and short summary of the research paper:\n",
      "\n",
      "The Transformer model is introduced, which uses self-attention mechanisms to compute representations of input and output sequences without relying on sequence-aligned RNNs or convolution. The paper highlights the advantages of self-attention, which reduces the number of operations required to relate signals from distant positions, making it easier to learn dependencies between positions. The Transformer's architecture is described, which consists of an encoder-decoder structure that maps input sequences to continuous representations and then generates output sequences one element at a time.\n",
      "\n",
      "Here is a concise and short summary of the research paper:\n",
      "\n",
      "The paper describes the architecture of the Transformer model, a neural network that generates output sequences one symbol at a time. The model consists of an encoder and a decoder, with the encoder converting input sequences into continuous representations and the decoder generating output symbols using a stack of self-attention and fully connected layers.\n",
      "\n",
      "Here is a concise and short summary of the research paper:\n",
      "\n",
      "The paper describes the architecture of the Transformer model, a neural network designed for sequence-to-sequence tasks. The model consists of an encoder and a decoder, each composed of multiple identical layers. The encoder layers use residual connections and layer normalization to process input sequences, while the decoder layers add a third sub-layer that performs multi-head attention over the output of the encoder. The attention mechanism is a key component, computing a weighted sum of values based on the compatibility of queries and keys. The paper specifically describes the \"Scaled Dot-Product Attention\" mechanism used in the model.\n",
      "\n",
      "Here is a concise and short summary of the research paper on Scaled Dot-Product Attention and Multi-Head Attention:\n",
      "\n",
      "The paper introduces Scaled Dot-Product Attention, a method that computes attention weights by taking the dot product of query and key vectors, dividing by the square root of the key vector's dimension, and applying a softmax function. This method is faster and more space-efficient than additive attention. The paper also proposes Multi-Head Attention, which involves projecting query, key, and value vectors multiple times with learned linear projections and then applying attention in parallel. This approach allows for more attention mechanisms to be used simultaneously, improving performance.\n",
      "\n",
      "Here is a concise and short summary of the research paper:\n",
      "\n",
      "The paper describes a multi-head attention mechanism in a neural network model. It combines queries, keys, and values and applies attention in parallel, producing high-dimensional output values. This allows the model to attend to different subspace representations at different positions, unlike a single attention head which averages out this information. Additionally, the paper provides mathematical explanations for why the dot products between queries and keys can result in large values.\n",
      "\n",
      "Here is a concise and short summary of the research paper:\n",
      "\n",
      "The paper describes the architecture of the Transformer model, a neural network designed for sequence-to-sequence tasks. The model uses multi-head attention, which is a combination of multiple attention mechanisms, to process input sequences. There are three types of attention used in the model: encoder-decoder attention, self-attention in the encoder, and self-attention in the decoder. The model also includes position-wise feed-forward networks, which are fully connected networks applied to each position separately. The model's architecture is designed to efficiently process long sequences and capture complex dependencies between input elements.\n",
      "\n",
      "Here is a concise and short summary of the research paper:\n",
      "\n",
      "The paper describes a sequence transduction model that uses a connected feed-forward network (FFN) to process each position in the input sequence separately. The FFN consists of two linear transformations with a ReLU activation in between. The model also uses learned embeddings to convert input and output tokens to vectors, and a softmax function to predict next-token probabilities. Additionally, the model injects positional information into the input embeddings using positional encoding to capture the order of the sequence.\n",
      "\n",
      "Here is a concise and short summary of the research paper:\n",
      "\n",
      "The paper analyzes the complexity and computational requirements of different neural network layer types, specifically self-attention, recurrent, and convolutional layers. It presents a comparison of these layers, highlighting the advantages of self-attention layers in processing variable-length sequences. The paper also introduces a novel positional encoding scheme using sine and cosine functions, which allows the model to learn to attend by relative positions and potentially extrapolate to longer sequence lengths.\n",
      "\n",
      "Here is a concise and short summary of the research paper:\n",
      "\n",
      "The paper compares the computational performance of self-attention and recurrent layers in sequence transduction tasks, specifically focusing on their ability to learn long-range dependencies. Self-attention layers are found to be faster and more parallelizable than recurrent layers, especially for shorter sequences. The paper also highlights the importance of path length between long-range dependencies, suggesting that self-attention layers can facilitate easier learning of these dependencies due to their shorter path lengths.\n",
      "\n",
      "Here is a concise and short summary of the research paper:\n",
      "\n",
      "The paper explores the use of self-attention in neural machine translation, comparing its performance to convolutional layers. They find that self-attention is more efficient and can be combined with point-wise feed-forward layers to achieve similar results. The paper also discusses the training process, including the use of the WMT 2014 English-German and English-French datasets and a batching scheme.\n",
      "\n",
      "Here is a concise and short summary of the research paper:\n",
      "\n",
      "The paper describes the training process of a machine learning model. The model was trained on a dataset of 25,000 source tokens and 25,000 target tokens using 8 NVIDIA P100 GPUs. The base model was trained for 12 hours, while the larger model was trained for 3.5 days. The training process used the Adam optimizer with a varying learning rate and employed three types of regularization, including residual dropout, to prevent overfitting.\n",
      "\n",
      "Here is a concise and short summary of the research paper:\n",
      "\n",
      "The paper presents a new state-of-the-art model for machine translation, the Transformer, which outperforms previous models on English-to-German and English-to-French translation tasks. The Transformer achieves a BLEU score of 28.4 on English-to-German and 41.0 on English-to-French, while requiring a fraction of the training cost of previous models. The paper also explores various techniques, including label smoothing and ensemble models, to improve the performance of the Transformer.\n",
      "\n",
      "Here is a concise and short summary of the research paper:\n",
      "\n",
      "The paper presents the results of various experiments on English-to-German translation using the Transformer model. The authors used several hyperparameters, including averaging multiple model checkpoints and beam search, to optimize performance. They also varied different components of the Transformer model to evaluate their importance, such as the number of attention heads. The results are presented in tables, showing improvements in translation quality and comparisons with other model architectures.\n",
      "\n",
      "Here is a concise and short summary of the research paper:\n",
      "\n",
      "The paper explores variations of the Transformer architecture, a novel sequence transduction model based on attention, and evaluates its performance on English-to-German translation. The authors experiment with different hyperparameters, including attention key size, model size, dropout rate, and positional embedding. The results show that reducing attention key size hurts model quality, while bigger models and dropout are beneficial. Additionally, replacing sinusoidal positional encoding with learned positional embeddings yields nearly identical results. The Transformer achieves a new state-of-the-art performance on English-to-German and English-to-French translation tasks, outperforming previous results, including ensembles.\n",
      "\n",
      "Here is a concise and short summary of the research paper:\n",
      "\n",
      "The authors achieved a new state-of-the-art result in English-to-French translation tasks, outperforming all previously reported ensemble models. They plan to apply attention-based models to other tasks, extend the Transformer to non-text modalities (images, audio, video), and investigate local attention mechanisms for efficient handling of large inputs and outputs.\n",
      "\n",
      "This research paper does not appear to be a standalone document, but rather a list of references cited in a research paper. The references are a mix of academic papers and preprints, published between 1997 and 2017, and cover topics such as neural machine translation, recurrent neural networks, and deep learning.\n",
      "\n",
      "If you would like, I can try to provide a brief summary of the research papers that are directly related to the field of natural language processing and machine translation. However, without more context or information about the specific research paper, it is difficult to provide a concise and short summary.\n",
      "\n",
      "The references appear to be a mix of foundational works on recurrent neural networks, long short-term memory networks, and sequence-to-sequence learning, as well as more recent papers on neural machine translation, convolutional sequence-to-sequence learning, and deep residual learning.\n",
      "\n",
      "Unfortunately, there is no research paper provided to summarize. The text appears to be a list of references to other research papers, with no abstract or summary provided for the original paper. If you'd like, I can help you write a summary for one of the referenced papers, or provide a general overview of the topics and themes mentioned in the references. Let me know!\n",
      "\n",
      "Here is a concise and short summary of the research papers:\n",
      "\n",
      "The papers listed summarize various advancements in neural networks, particularly in the areas of language models, machine translation, and natural language processing.\n",
      "\n",
      "* [21-22] Explore attention-based neural machine translation and decomposable attention models.\n",
      "* [23] Introduces a deep reinforced model for abstractive summarization.\n",
      "* [24] Improves language models using output embeddings.\n",
      "* [25] Introduces subword units for neural machine translation of rare words.\n",
      "* [26] Presents a sparsely-gated mixture-of-experts layer for large neural networks.\n",
      "* [27] Introduces dropout as a method to prevent neural networks from overfitting.\n",
      "* [28] Describes end-to-end memory networks for sequence-to-sequence tasks.\n",
      "* [29] Introduces sequence-to-sequence learning with neural networks.\n",
      "* [30-31] Discuss various advancements in computer vision and neural networks, including the inception architecture.\n",
      "\n",
      "These papers contribute to the development of more effective neural network architectures, techniques, and applications in various domains.\n",
      "\n",
      "Here is a concise and short summary of the two research papers:\n",
      "\n",
      "**Paper 1: \"Rethinking the Inception Architecture for Computer Vision\" (2015)**\n",
      "\n",
      "This paper proposes a new architecture for convolutional neural networks (CNNs), called Inception-V3, which improves the performance of image classification tasks. The authors introduce a novel way to combine different scales and resolutions of feature hierarchies, allowing the network to capture more detailed and abstract features.\n",
      "\n",
      "**Paper 2: \"Google's Neural Machine Translation System\" (2016) and \"Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation\" (2016)**\n",
      "\n",
      "These two papers focus on neural machine translation (NMT) systems. The first paper describes Google's NMT system, which uses a sequence-to-sequence model with attention mechanisms to translate text from one language to another. The second paper presents a new architecture for NMT, called Deep Recurrent Models with Fast-Forward Connections (DRMM), which uses a combination of recurrent and convolutional neural networks to improve translation accuracy and efficiency.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'**Title:** \"Attention Is All You Need: A New Neural Network Architecture for Sequence-to-Sequence Tasks\"\\n\\n**Summary:** This research paper proposes a new neural network architecture called the Transformer, which achieves state-of-the-art results in machine translation tasks without using recurrent or convolutional neural networks. The Transformer relies solely on attention mechanisms to process input sequences, allowing for more parallelization and significant improvements in computational efficiency. The model achieves a new state-of-the-art BLEU score of 41.0 on the WMT 2014 English-to-French translation task.\\n\\n**Applications:** The Transformer architecture can be used in various natural language processing applications, such as:\\n\\n* Neural machine translation: The Transformer\\'s ability to process input sequences in parallel makes it an attractive solution for machine translation tasks, particularly for languages with complex grammar and syntax.\\n* Text summarization: The Transformer\\'s attention mechanism can be used to identify the most important parts of a text and generate a summary.\\n* Language modeling: The Transformer\\'s ability to process input sequences in parallel can be used to improve language modeling tasks, such as language generation and language understanding.\\n\\n**Key Components:**\\n\\n* Self-attention mechanisms: The Transformer uses self-attention mechanisms to compute representations of input and output sequences without relying on sequence-aligned RNNs or convolution.\\n* Multi-head attention: The Transformer uses multi-head attention to attend to different subspace representations at different positions, allowing for more flexibility and accuracy in modeling dependencies between input elements.\\n* Position-wise feed-forward networks: The Transformer uses position-wise feed-forward networks to process each position in the input sequence separately, allowing for more parallelization and efficiency.\\n\\n**Advantages:**\\n\\n* Improved computational efficiency: The Transformer\\'s ability to process input sequences in parallel makes it more efficient than traditional recurrent neural networks.\\n* Better handling of long-range dependencies: The Transformer\\'s self-attention mechanisms allow it to model dependencies between input elements more effectively, even for long-range dependencies.\\n* Improved parallelization: The Transformer\\'s parallelization capabilities make it more suitable for large-scale machine translation tasks.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_chain = load_summarize_chain(llm = llm,\n",
    "                                     chain_type = 'map_reduce',\n",
    "                                     map_prompt = map_prompt_template,\n",
    "                                     combine_prompt = final_prompt_template,\n",
    "                                     verbose = True\n",
    "                                     )\n",
    "summary = summary_chain.run(docs)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗†\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Łukasz Kaiser ∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring signiﬁcantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.0 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature.\n",
      "1 Introduction\n",
      "Recurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\n",
      "in particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\n",
      "transduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\n",
      "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder\n",
      "architectures [31, 21, 13].\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The paper \"Attention Is All You Need\" proposes a new neural network architecture called the Transformer, which uses attention mechanisms to process sequences without relying on recurrence or convolution. The Transformer outperforms existing state-of-the-art models on two machine translation tasks, achieving a new single-model state-of-the-art BLEU score of 41.0 on the WMT 2014 English-to-French translation task. The model requires significantly less training time and is more parallelizable than previous models.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "architectures [31, 21, 13].\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
      "our research.\n",
      "†Work performed while at Google Brain.\n",
      "‡Work performed while at Google Research.\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Based on the provided context, I will refine the original summary to include more information about the authors' contributions to the development of the Transformer model.\n",
      "\n",
      "Original Summary:\n",
      "\n",
      "The paper \"Attention Is All You Need\" proposes a new neural network architecture called the Transformer, which uses attention mechanisms to process sequences without relying on recurrence or convolution. The Transformer outperforms existing state-of-the-art models on two machine translation tasks, achieving a new single-model state-of-the-art BLEU score of 41.0 on the WMT 2014 English-to-French translation task. The model requires significantly less training time and is more parallelizable than previous models.\n",
      "\n",
      "Refined Summary:\n",
      "\n",
      "The paper \"Attention Is All You Need\" proposes a new neural network architecture called the Transformer, which uses attention mechanisms to process sequences without relying on recurrence or convolution. The Transformer outperforms existing state-of-the-art models on two machine translation tasks, achieving a new single-model state-of-the-art BLEU score of 41.0 on the WMT 2014 English-to-French translation task. The model requires significantly less training time and is more parallelizable than previous models. The authors, Jakob, Ashish, Noam, Niki, Llion, Lukasz, and Aidan, made significant contributions to the development of the Transformer model. Jakob initially proposed replacing RNNs with self-attention, while Ashish and Illia designed and implemented the first Transformer models. Noam contributed key ideas such as scaled dot-product attention, multi-head attention, and parameter-free position representation. Niki, Llion, Lukasz, and Aidan also made important contributions, including designing and implementing model variants, accelerating research, and improving results.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output\n",
      "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\n",
      "states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\n",
      "sequential nature precludes parallelization within training examples, which becomes critical at longer\n",
      "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved\n",
      "signiﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\n",
      "computation [26], while also improving model performance in case of the latter. The fundamental\n",
      "constraint of sequential computation, however, remains.\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\n",
      "block, computing hidden representations in parallel for all input and output positions. In these models,\n",
      "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Based on the provided context, I will refine the original summary to include more information about the authors' contributions to the development of the Transformer model.\n",
      "\n",
      "Original Summary:\n",
      "\n",
      "The paper \"Attention Is All You Need\" proposes a new neural network architecture called the Transformer, which uses attention mechanisms to process sequences without relying on recurrence or convolution. The Transformer outperforms existing state-of-the-art models on two machine translation tasks, achieving a new single-model state-of-the-art BLEU score of 41.0 on the WMT 2014 English-to-French translation task. The model requires significantly less training time and is more parallelizable than previous models.\n",
      "\n",
      "Refined Summary:\n",
      "\n",
      "The paper \"Attention Is All You Need\" proposes a new neural network architecture called the Transformer, which uses attention mechanisms to process sequences without relying on recurrence or convolution. The Transformer outperforms existing state-of-the-art models on two machine translation tasks, achieving a new single-model state-of-the-art BLEU score of 41.0 on the WMT 2014 English-to-French translation task. The model requires significantly less training time and is more parallelizable than previous models. The authors, Jakob, Ashish, Noam, Niki, Llion, Lukasz, and Aidan, made significant contributions to the development of the Transformer model. Jakob initially proposed replacing RNNs with self-attention, while Ashish and Illia designed and implemented the first Transformer models. Noam contributed key ideas such as scaled dot-product attention, multi-head attention, and parameter-free position representation. Niki, Llion, Lukasz, and Aidan also made important contributions, including designing and implementing model variants, accelerating research, and improving results.\n",
      "\n",
      "With the additional context, I refined the summary to include more information about the authors' contributions to the development of the Transformer model. The context highlights the limitations of recurrent models, including their sequential nature and the need for parallelization. It also mentions the use of attention mechanisms in sequence modeling and the authors' proposal of the Transformer model, which eschews recurrence and relies entirely on attention mechanisms. The context provides more insight into the authors' work and their contributions to the development of the Transformer model.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "the number of operations required to relate signals from two arbitrary input or output positions grows\n",
      "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\n",
      "it more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 22, 23, 19].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [28].\n",
      "To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [14, 15] and [8].\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\n",
      "Here, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\n",
      "of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output\n",
      "sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Refined Summary:\n",
      "\n",
      "The paper \"Attention Is All You Need\" proposes a new neural network architecture called the Transformer, which uses attention mechanisms to process sequences without relying on recurrence or convolution. The Transformer outperforms existing state-of-the-art models on two machine translation tasks, achieving a new single-model state-of-the-art BLEU score of 41.0 on the WMT 2014 English-to-French translation task. The model requires significantly less training time and is more parallelizable than previous models. The authors, Jakob, Ashish, Noam, Niki, Llion, Lukasz, and Aidan, made significant contributions to the development of the Transformer model. Jakob initially proposed replacing RNNs with self-attention, while Ashish and Illia designed and implemented the first Transformer models. Noam contributed key ideas such as scaled dot-product attention, multi-head attention, and parameter-free position representation. Niki, Llion, Lukasz, and Aidan also made important contributions, including designing and implementing model variants, accelerating research, and improving results.\n",
      "\n",
      "The Transformer's architecture is designed to overcome the limitations of recurrent models, which require sequential processing and struggle to learn dependencies between distant positions. In contrast, the Transformer uses self-attention mechanisms to relate different positions of a single sequence, reducing the number of operations required to learn dependencies between distant positions to a constant number. The authors demonstrate the effectiveness of the Transformer model, which achieves state-of-the-art results on machine translation tasks while requiring significantly less training time and being more parallelizable than previous models.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "of continuous representations z = (z1,...,z n). Given z, the decoder then generates an output\n",
      "sequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\n",
      "[9], consuming the previously generated symbols as additional input when generating the next.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "2\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Refined Summary:\n",
      "\n",
      "The paper \"Attention Is All You Need\" proposes a new neural network architecture called the Transformer, which uses attention mechanisms to process sequences without relying on recurrence or convolution. The Transformer outperforms existing state-of-the-art models on two machine translation tasks, achieving a new single-model state-of-the-art BLEU score of 41.0 on the WMT 2014 English-to-French translation task. The model requires significantly less training time and is more parallelizable than previous models.\n",
      "\n",
      "The Transformer's architecture is designed to overcome the limitations of recurrent models, which require sequential processing and struggle to learn dependencies between distant positions. The Transformer uses self-attention mechanisms to relate different positions of a single sequence, reducing the number of operations required to learn dependencies between distant positions to a constant number.\n",
      "\n",
      "The Transformer follows a specific architecture, consisting of an encoder and a decoder. The encoder is composed of a stack of 6 identical layers, each containing a multi-head self-attention mechanism and a simple, position-wise fully connected layer. The decoder also consists of a stack of identical layers, with a similar architecture. The decoder generates an output sequence of symbols one element at a time, using the encoder's output to predict the next symbol.\n",
      "\n",
      "The authors, Jakob, Ashish, Noam, Niki, Llion, Lukasz, and Aidan, made significant contributions to the development of the Transformer model. Jakob initially proposed replacing RNNs with self-attention, while Ashish and Illia designed and implemented the first Transformer models. Noam contributed key ideas such as scaled dot-product attention, multi-head attention, and parameter-free position representation. Niki, Llion, Lukasz, and Aidan also made important contributions, including designing and implementing model variants, accelerating research, and improving results.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Figure 1: The Transformer - model architecture.\n",
      "wise fully connected feed-forward network. We employ a residual connection [10] around each of\n",
      "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
      "LayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\n",
      "itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\n",
      "layers, produce outputs of dimension dmodel = 512.\n",
      "Decoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\n",
      "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\n",
      "attention over the output of the encoder stack. Similar to the encoder, we employ residual connections\n",
      "around each of the sub-layers, followed by layer normalization. We also modify the self-attention\n",
      "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\n",
      "masking, combined with fact that the output embeddings are offset by one position, ensures that the\n",
      "predictions for position ican depend only on the known outputs at positions less than i.\n",
      "3.2 Attention\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
      "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
      "query with the corresponding key.\n",
      "3.2.1 Scaled Dot-Product Attention\n",
      "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\n",
      "queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\n",
      "3\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Refined Summary:\n",
      "\n",
      "The paper \"Attention Is All You Need\" proposes a new neural network architecture called the Transformer, which uses attention mechanisms to process sequences without relying on recurrence or convolution. The Transformer's architecture is designed to overcome the limitations of recurrent models, which require sequential processing and struggle to learn dependencies between distant positions.\n",
      "\n",
      "The Transformer's architecture consists of an encoder and a decoder. The encoder is composed of a stack of 6 identical layers, each containing a multi-head self-attention mechanism and a simple, position-wise fully connected feed-forward network (FFNN) with residual connections and layer normalization. The decoder also consists of a stack of identical layers, with a similar architecture. The decoder generates an output sequence of symbols one element at a time, using the encoder's output to predict the next symbol.\n",
      "\n",
      "The Transformer uses a scaled dot-product attention mechanism, which computes the output as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. This mechanism is designed to prevent positions from attending to subsequent positions, ensuring that the predictions for position i can only depend on the known outputs at positions less than i.\n",
      "\n",
      "The authors made significant contributions to the development of the Transformer model, including key ideas such as scaled dot-product attention, multi-head attention, and parameter-free position representation. The model outperforms existing state-of-the-art models on two machine translation tasks, achieving a new single-model state-of-the-art BLEU score of 41.0 on the WMT 2014 English-to-French translation task. The model also requires significantly less training time and is more parallelizable than previous models.\n",
      "\n",
      "Overall, the Transformer's innovative architecture and attention mechanisms enable it to process sequences more efficiently and effectively, achieving state-of-the-art results in machine translation tasks.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Scaled Dot-Product Attention\n",
      " Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\n",
      "attention layers running in parallel.\n",
      "query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\n",
      "values.\n",
      "In practice, we compute the attention function on a set of queries simultaneously, packed together\n",
      "into a matrix Q. The keys and values are also packed together into matrices Kand V. We compute\n",
      "the matrix of outputs as:\n",
      "Attention(Q,K,V ) = softmax(QKT\n",
      "√dk\n",
      ")V (1)\n",
      "The two most commonly used attention functions are additive attention [2], and dot-product (multi-\n",
      "plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n",
      "of 1√dk\n",
      ". Additive attention computes the compatibility function using a feed-forward network with\n",
      "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\n",
      "much faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\n",
      "matrix multiplication code.\n",
      "While for small values of dk the two mechanisms perform similarly, additive attention outperforms\n",
      "dot product attention without scaling for larger values of dk [3]. We suspect that for large values of\n",
      "dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\n",
      "extremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\n",
      ".\n",
      "3.2.2 Multi-Head Attention\n",
      "Instead of performing a single attention function with dmodel-dimensional keys, values and queries,\n",
      "we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\n",
      "linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\n",
      "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Refined Summary:\n",
      "\n",
      "The paper \"Attention Is All You Need\" proposes the Transformer, a neural network architecture that uses attention mechanisms to process sequences without relying on recurrence or convolution. The Transformer's architecture consists of an encoder and a decoder, both composed of identical layers featuring multi-head self-attention mechanisms and position-wise fully connected feed-forward networks (FFNNs) with residual connections and layer normalization.\n",
      "\n",
      "The Transformer employs a scaled dot-product attention mechanism, which computes the output as a weighted sum of values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. This mechanism is designed to prevent positions from attending to subsequent positions, ensuring that predictions for position i can only depend on the known outputs at positions less than i.\n",
      "\n",
      "The paper introduces two key ideas: scaled dot-product attention and multi-head attention. Scaled dot-product attention computes the compatibility function by dividing each dot product by √dk and applying a softmax function to obtain the weights on the values. This mechanism outperforms additive attention for larger values of dk. Multi-head attention linearly projects the queries, keys, and values multiple times with different learned linear projections, and then performs attention functions in parallel on each projected version.\n",
      "\n",
      "The Transformer achieves state-of-the-art results on two machine translation tasks, outperforming existing models with a new single-model state-of-the-art BLEU score of 41.0 on the WMT 2014 English-to-French translation task. The model also requires significantly less training time and is more parallelizable than previous models. Overall, the Transformer's innovative architecture and attention mechanisms enable it to process sequences more efficiently and effectively, achieving state-of-the-art results in machine translation tasks.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n",
      "output values. These are concatenated and once again projected, resulting in the ﬁnal values, as\n",
      "depicted in Figure 2.\n",
      "Multi-head attention allows the model to jointly attend to information from different representation\n",
      "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "4To illustrate why the dot products get large, assume that the components of q and k are independent random\n",
      "variables with mean 0 and variance 1. Then their dot product, q · k = ∑dk\n",
      "i=1 qiki, has mean 0 and variance dk.\n",
      "4\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Refined Summary:\n",
      "\n",
      "The paper \"Attention Is All You Need\" proposes the Transformer, a neural network architecture that utilizes attention mechanisms to process sequences without relying on recurrence or convolution. The Transformer's architecture consists of an encoder and a decoder, both composed of identical layers featuring multi-head self-attention mechanisms and position-wise fully connected feed-forward networks (FFNNs) with residual connections and layer normalization.\n",
      "\n",
      "The Transformer employs a scaled dot-product attention mechanism, which computes the output as a weighted sum of values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. This mechanism is designed to prevent positions from attending to subsequent positions, ensuring that predictions for position i can only depend on the known outputs at positions less than i.\n",
      "\n",
      "The paper introduces two key ideas: scaled dot-product attention and multi-head attention. Scaled dot-product attention computes the compatibility function by dividing each dot product by √dk and applying a softmax function to obtain the weights on the values. This mechanism outperforms additive attention for larger values of dk. Multi-head attention linearly projects the queries, keys, and values multiple times with different learned linear projections, and then performs attention functions in parallel on each projected version. This allows the model to jointly attend to information from different representation subspaces at different positions, as depicted in Figure 2. The output values from each attention head are concatenated and projected again to yield the final values.\n",
      "\n",
      "The Transformer achieves state-of-the-art results on two machine translation tasks, outperforming existing models with a new single-model state-of-the-art BLEU score of 41.0 on the WMT 2014 English-to-French translation task. The model also requires significantly less training time and is more parallelizable than previous models. Overall, the Transformer's innovative architecture and attention mechanisms enable it to process sequences more efficiently and effectively, achieving state-of-the-art results in machine translation tasks.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\n",
      "where headi = Attention(QWQ\n",
      "i ,KW K\n",
      "i ,VW V\n",
      "i )\n",
      "Where the projections are parameter matricesWQ\n",
      "i ∈Rdmodel×dk , WK\n",
      "i ∈Rdmodel×dk , WV\n",
      "i ∈Rdmodel×dv\n",
      "and WO ∈Rhdv×dmodel .\n",
      "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
      "dk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\n",
      "is similar to that of single-head attention with full dimensionality.\n",
      "3.2.3 Applications of Attention in our Model\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\n",
      "and the memory keys and values come from the output of the encoder. This allows every\n",
      "position in the decoder to attend over all positions in the input sequence. This mimics the\n",
      "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n",
      "[31, 2, 8].\n",
      "• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n",
      "and queries come from the same place, in this case, the output of the previous layer in the\n",
      "encoder. Each position in the encoder can attend to all positions in the previous layer of the\n",
      "encoder.\n",
      "• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n",
      "all positions in the decoder up to and including that position. We need to prevent leftward\n",
      "information ﬂow in the decoder to preserve the auto-regressive property. We implement this\n",
      "inside of scaled dot-product attention by masking out (setting to −∞) all values in the input\n",
      "of the softmax which correspond to illegal connections. See Figure 2.\n",
      "3.3 Position-wise Feed-Forward Networks\n",
      "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
      "connected feed-forward network, which is applied to each position separately and identically. This\n",
      "consists of two linear transformations with a ReLU activation in between.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Refined Summary:\n",
      "\n",
      "The paper \"Attention Is All You Need\" introduces the Transformer, a neural network architecture that processes sequences without recurrence or convolution, relying solely on attention mechanisms. The Transformer's architecture consists of an encoder and a decoder, both composed of identical layers featuring multi-head self-attention mechanisms and position-wise fully connected feed-forward networks (FFNNs) with residual connections and layer normalization.\n",
      "\n",
      "The Transformer employs a novel scaled dot-product attention mechanism, which computes the output as a weighted sum of values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. This mechanism ensures that predictions for position i can only depend on the known outputs at positions less than i.\n",
      "\n",
      "The paper introduces two key ideas: scaled dot-product attention and multi-head attention. The latter computes attention in parallel on each projected version of the queries, keys, and values, allowing the model to jointly attend to information from different representation subspaces at different positions. The output values from each attention head are concatenated and projected again to yield the final values.\n",
      "\n",
      "The Transformer achieves state-of-the-art results on two machine translation tasks, outperforming existing models with a new single-model state-of-the-art BLEU score of 41.0 on the WMT 2014 English-to-French translation task. The model also requires significantly less training time and is more parallelizable than previous models.\n",
      "\n",
      "In addition to the new insights, the paper reveals the architecture's components in greater detail. The multi-head attention mechanism is defined as Concat(head1,..., headh)WO, where headi = Attention(QWQ\n",
      "i ,KW K\n",
      "i ,VW V\n",
      "i ). The Transformer employs 8 parallel attention layers, or heads, with reduced dimensionality to maintain computational efficiency. The model uses multi-head attention in three ways: encoder-decoder attention, self-attention in the encoder, and self-attention in the decoder, with leftward information flow prevention in the decoder.\n",
      "\n",
      "Furthermore, each layer in the encoder and decoder contains a position-wise fully connected feed-forward network, consisting of two linear transformations with a ReLU activation in between. This architecture enables the Transformer to process sequences more efficiently and effectively, achieving state-of-the-art results in machine translation tasks.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "connected feed-forward network, which is applied to each position separately and identically. This\n",
      "consists of two linear transformations with a ReLU activation in between.\n",
      "FFN(x) = max(0,xW1 + b1)W2 + b2 (2)\n",
      "While the linear transformations are the same across different positions, they use different parameters\n",
      "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
      "The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\n",
      "dff = 2048.\n",
      "3.4 Embeddings and Softmax\n",
      "Similarly to other sequence transduction models, we use learned embeddings to convert the input\n",
      "tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\n",
      "mation and softmax function to convert the decoder output to predicted next-token probabilities. In\n",
      "our model, we share the same weight matrix between the two embedding layers and the pre-softmax\n",
      "linear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.\n",
      "3.5 Positional Encoding\n",
      "Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
      "order of the sequence, we must inject some information about the relative or absolute position of the\n",
      "tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
      "5\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Here is the refined summary:\n",
      "\n",
      "The paper \"Attention Is All You Need\" introduces the Transformer, a neural network architecture that processes sequences without recurrence or convolution, relying solely on attention mechanisms. The Transformer's architecture consists of an encoder and a decoder, both composed of identical layers featuring multi-head self-attention mechanisms and position-wise fully connected feed-forward networks (FFNNs) with residual connections and layer normalization.\n",
      "\n",
      "The Transformer employs a novel scaled dot-product attention mechanism, which computes the output as a weighted sum of values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. This mechanism ensures that predictions for position i can only depend on the known outputs at positions less than i.\n",
      "\n",
      "The paper introduces two key ideas: scaled dot-product attention and multi-head attention. The latter computes attention in parallel on each projected version of the queries, keys, and values, allowing the model to jointly attend to information from different representation subspaces at different positions. The output values from each attention head are concatenated and projected again to yield the final values.\n",
      "\n",
      "Each layer in the encoder and decoder contains a position-wise fully connected feed-forward network (FFNN), which is applied to each position separately and identically. This FFNN consists of two linear transformations with a ReLU activation in between. Additionally, the model uses learned embeddings to convert input and output tokens to vectors, and positional encoding is added to the input embeddings to inject information about the relative or absolute position of the tokens in the sequence.\n",
      "\n",
      "The Transformer achieves state-of-the-art results on two machine translation tasks, outperforming existing models with a new single-model state-of-the-art BLEU score of 41.0 on the WMT 2014 English-to-French translation task. The model also requires significantly less training time and is more parallelizable than previous models.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\n",
      "for different layer types. nis the sequence length, dis the representation dimension, kis the kernel\n",
      "size of convolutions and rthe size of the neighborhood in restricted self-attention.\n",
      "Layer Type Complexity per Layer Sequential Maximum Path Length\n",
      "Operations\n",
      "Self-Attention O(n2 ·d) O(1) O(1)\n",
      "Recurrent O(n·d2) O(n) O(n)\n",
      "Convolutional O(k·n·d2) O(1) O(logk(n))\n",
      "Self-Attention (restricted) O(r·n·d) O(1) O(n/r)\n",
      "bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
      "as the embeddings, so that the two can be summed. There are many choices of positional encodings,\n",
      "learned and ﬁxed [8].\n",
      "In this work, we use sine and cosine functions of different frequencies:\n",
      "PE(pos,2i) = sin(pos/100002i/dmodel )\n",
      "PE(pos,2i+1) = cos(pos/100002i/dmodel )\n",
      "where posis the position and iis the dimension. That is, each dimension of the positional encoding\n",
      "corresponds to a sinusoid. The wavelengths form a geometric progression from 2πto 10000 ·2π. We\n",
      "chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
      "relative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of\n",
      "PEpos.\n",
      "We also experimented with using learned positional embeddings [8] instead, and found that the two\n",
      "versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\n",
      "because it may allow the model to extrapolate to sequence lengths longer than the ones encountered\n",
      "during training.\n",
      "4 Why Self-Attention\n",
      "In this section we compare various aspects of self-attention layers to the recurrent and convolu-\n",
      "tional layers commonly used for mapping one variable-length sequence of symbol representations\n",
      "(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden\n",
      "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Here is the refined summary:\n",
      "\n",
      "The paper \"Attention Is All You Need\" introduces the Transformer, a neural network architecture that processes sequences without recurrence or convolution, relying solely on attention mechanisms. The Transformer's architecture consists of an encoder and a decoder, both composed of identical layers featuring multi-head self-attention mechanisms and position-wise fully connected feed-forward networks (FFNNs) with residual connections and layer normalization.\n",
      "\n",
      "The Transformer employs a novel scaled dot-product attention mechanism, which computes the output as a weighted sum of values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. This mechanism ensures that predictions for position i can only depend on the known outputs at positions less than i.\n",
      "\n",
      "The paper introduces two key ideas: scaled dot-product attention and multi-head attention. The latter computes attention in parallel on each projected version of the queries, keys, and values, allowing the model to jointly attend to information from different representation subspaces at different positions. The output values from each attention head are concatenated and projected again to yield the final values.\n",
      "\n",
      "Each layer in the encoder and decoder contains a position-wise fully connected feed-forward network (FFNN), which is applied to each position separately and identically. This FFNN consists of two linear transformations with a ReLU activation in between. Additionally, the model uses learned embeddings to convert input and output tokens to vectors, and positional encoding is added to the input embeddings to inject information about the relative or absolute position of the tokens in the sequence.\n",
      "\n",
      "The Transformer also uses a novel approach to positional encoding, where sine and cosine functions of different frequencies are used to represent the position of each token in the sequence. This allows the model to easily learn to attend by relative positions and enables it to extrapolate to sequence lengths longer than those encountered during training.\n",
      "\n",
      "The Transformer achieves state-of-the-art results on two machine translation tasks, outperforming existing models with a new single-model state-of-the-art BLEU score of 41.0 on the WMT 2014 English-to-French translation task. The model also requires significantly less training time and is more parallelizable than previous models.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden\n",
      "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\n",
      "consider three desiderata.\n",
      "One is the total computational complexity per layer. Another is the amount of computation that can\n",
      "be parallelized, as measured by the minimum number of sequential operations required.\n",
      "The third is the path length between long-range dependencies in the network. Learning long-range\n",
      "dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\n",
      "ability to learn such dependencies is the length of the paths forward and backward signals have to\n",
      "traverse in the network. The shorter these paths between any combination of positions in the input\n",
      "and output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\n",
      "the maximum path length between any two input and output positions in networks composed of the\n",
      "different layer types.\n",
      "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\n",
      "executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\n",
      "computational complexity, self-attention layers are faster than recurrent layers when the sequence\n",
      "length n is smaller than the representation dimensionality d, which is most often the case with\n",
      "sentence representations used by state-of-the-art models in machine translations, such as word-piece\n",
      "[31] and byte-pair [25] representations. To improve computational performance for tasks involving\n",
      "very long sequences, self-attention could be restricted to considering only a neighborhood of size rin\n",
      "6\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Here is the refined summary:\n",
      "\n",
      "The paper \"Attention Is All You Need\" presents the Transformer, a novel neural network architecture that processes sequences without recurrence or convolution, relying solely on attention mechanisms. The Transformer consists of an encoder and a decoder, each composed of identical layers featuring multi-head self-attention mechanisms and position-wise fully connected feed-forward networks (FFNNs) with residual connections and layer normalization. The Transformer employs a scaled dot-product attention mechanism, which computes the output as a weighted sum of values, and multi-head attention, which computes attention in parallel on each projected version of the queries, keys, and values.\n",
      "\n",
      "The paper introduces two key ideas: scaled dot-product attention and multi-head attention. Each layer in the encoder and decoder contains a position-wise fully connected feed-forward network (FFNN), which is applied to each position separately and identically. Additionally, the model uses learned embeddings to convert input and output tokens to vectors, and positional encoding is added to the input embeddings to inject information about the relative or absolute position of the tokens in the sequence.\n",
      "\n",
      "The Transformer uses a novel approach to positional encoding, where sine and cosine functions of different frequencies are used to represent the position of each token in the sequence. This allows the model to easily learn to attend by relative positions and enables it to extrapolate to sequence lengths longer than those encountered during training.\n",
      "\n",
      "The Transformer achieves state-of-the-art results on two machine translation tasks, outperforming existing models with a new single-model state-of-the-art BLEU score of 41.0 on the WMT 2014 English-to-French translation task. The model also requires significantly less training time and is more parallelizable than previous models. The motivation behind the use of self-attention is to reduce the computational complexity per layer, increase parallelization, and shorten the path length between long-range dependencies in the network, making it easier to learn long-range dependencies.\n",
      "\n",
      "Note that the new context provides additional information on the computational complexity and parallelization of self-attention layers, which is incorporated into the refined summary.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "the input sequence centered around the respective output position. This would increase the maximum\n",
      "path length to O(n/r). We plan to investigate this approach further in future work.\n",
      "A single convolutional layer with kernel width k<n does not connect all pairs of input and output\n",
      "positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\n",
      "or O(logk(n)) in the case of dilated convolutions [ 15], increasing the length of the longest paths\n",
      "between any two positions in the network. Convolutional layers are generally more expensive than\n",
      "recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\n",
      "considerably, to O(k·n·d+ n·d2). Even with k = n, however, the complexity of a separable\n",
      "convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\n",
      "the approach we take in our model.\n",
      "As side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\n",
      "from our models and present and discuss examples in the appendix. Not only do individual attention\n",
      "heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\n",
      "and semantic structure of the sentences.\n",
      "5 Training\n",
      "This section describes the training regime for our models.\n",
      "5.1 Training Data and Batching\n",
      "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\n",
      "sentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\n",
      "target vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\n",
      "2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\n",
      "vocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\n",
      "batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\n",
      "target tokens.\n",
      "5.2 Hardware and Schedule\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Here is the refined summary:\n",
      "\n",
      "The paper \"Attention Is All You Need\" introduces the Transformer, a novel neural network architecture that processes sequences without recurrence or convolution, relying solely on attention mechanisms. The Transformer consists of an encoder and a decoder, each composed of identical layers featuring multi-head self-attention mechanisms and position-wise fully connected feed-forward networks (FFNNs) with residual connections and layer normalization. The Transformer employs a scaled dot-product attention mechanism, which computes the output as a weighted sum of values, and multi-head attention, which computes attention in parallel on each projected version of the queries, keys, and values.\n",
      "\n",
      "The paper presents two key ideas: scaled dot-product attention and multi-head attention. Each layer in the encoder and decoder contains a position-wise fully connected feed-forward network (FFNN), which is applied to each position separately and identically. Additionally, the model uses learned embeddings to convert input and output tokens to vectors, and positional encoding is added to the input embeddings to inject information about the relative or absolute position of the tokens in the sequence.\n",
      "\n",
      "The Transformer uses a novel approach to positional encoding, where sine and cosine functions of different frequencies are used to represent the position of each token in the sequence. This allows the model to easily learn to attend by relative positions and enables it to extrapolate to sequence lengths longer than those encountered during training.\n",
      "\n",
      "The paper also provides insight into the computational complexity and parallelization of self-attention layers, which is more efficient than convolutional layers. Specifically, self-attention layers have a complexity of O(n), whereas convolutional layers have a complexity of O(k·n·d+ n·d2) even with k = n. Moreover, self-attention layers can be parallelized more easily, reducing the training time and making the model more scalable.\n",
      "\n",
      "The Transformer achieves state-of-the-art results on two machine translation tasks, outperforming existing models with a new single-model state-of-the-art BLEU score of 41.0 on the WMT 2014 English-to-French translation task. The model also requires significantly less training time and is more parallelizable than previous models. The motivation behind the use of self-attention is to reduce the computational complexity per layer, increase parallelization, and shorten the path length between long-range dependencies in the network, making it easier to learn long-range dependencies.\n",
      "\n",
      "Additionally, the paper demonstrates the potential of self-attention to yield more interpretable models, with individual attention heads learning to perform different tasks and exhibiting behavior related to the syntactic and semantic structure of the sentences.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\n",
      "target tokens.\n",
      "5.2 Hardware and Schedule\n",
      "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\n",
      "the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\n",
      "trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\n",
      "bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n",
      "(3.5 days).\n",
      "5.3 Optimizer\n",
      "We used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ= 10−9. We varied the learning\n",
      "rate over the course of training, according to the formula:\n",
      "lrate= d−0.5\n",
      "model ·min(step_num−0.5,step_num·warmup_steps−1.5) (3)\n",
      "This corresponds to increasing the learning rate linearly for the ﬁrst warmup_stepstraining steps,\n",
      "and decreasing it thereafter proportionally to the inverse square root of the step number. We used\n",
      "warmup_steps= 4000.\n",
      "5.4 Regularization\n",
      "We employ three types of regularization during training:\n",
      "Residual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the\n",
      "sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\n",
      "positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\n",
      "Pdrop = 0.1.\n",
      "7\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Here is the refined summary:\n",
      "\n",
      "The paper \"Attention Is All You Need\" introduces the Transformer, a novel neural network architecture that processes sequences without recurrence or convolution, relying solely on attention mechanisms. The Transformer consists of an encoder and a decoder, each composed of identical layers featuring multi-head self-attention mechanisms and position-wise fully connected feed-forward networks (FFNNs) with residual connections and layer normalization. The Transformer employs a scaled dot-product attention mechanism, which computes the output as a weighted sum of values, and multi-head attention, which computes attention in parallel on each projected version of the queries, keys, and values.\n",
      "\n",
      "The paper presents two key ideas: scaled dot-product attention and multi-head attention. Each layer in the encoder and decoder contains a position-wise fully connected feed-forward network (FFNN), which is applied to each position separately and identically. Additionally, the model uses learned embeddings to convert input and output tokens to vectors, and positional encoding is added to the input embeddings to inject information about the relative or absolute position of the tokens in the sequence.\n",
      "\n",
      "The Transformer uses a novel approach to positional encoding, where sine and cosine functions of different frequencies are used to represent the position of each token in the sequence. This allows the model to easily learn to attend by relative positions and enables it to extrapolate to sequence lengths longer than those encountered during training.\n",
      "\n",
      "The paper also provides insight into the computational complexity and parallelization of self-attention layers, which is more efficient than convolutional layers. Specifically, self-attention layers have a complexity of O(n), whereas convolutional layers have a complexity of O(k·n·d+ n·d2) even with k = n. Moreover, self-attention layers can be parallelized more easily, reducing the training time and making the model more scalable.\n",
      "\n",
      "The paper also provides details on the training process, including the use of a batch size of approximately 25,000 sentence pairs, training on 8 NVIDIA P100 GPUs, and the use of the Adam optimizer with a varying learning rate. The model was trained for a total of 100,000 steps (12 hours) for the base models and 300,000 steps (3.5 days) for the larger models. Additionally, the paper discusses the use of regularization techniques, including residual dropout, to prevent overfitting.\n",
      "\n",
      "The Transformer achieves state-of-the-art results on two machine translation tasks, outperforming existing models with a new single-model state-of-the-art BLEU score of 41.0 on the WMT 2014 English-to-French translation task. The model also requires significantly less training time and is more parallelizable than previous models. The motivation behind the use of self-attention is to reduce the computational complexity per layer, increase parallelization, and shorten the path length between long-range dependencies in the network, making it easier to learn long-range dependencies.\n",
      "\n",
      "Additionally, the paper demonstrates the potential of self-attention to yield more interpretable models, with individual attention heads learning to perform different tasks and exhibiting behavior related to the syntactic and semantic structure of the sentences.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\n",
      "English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\n",
      "Model\n",
      "BLEU Training Cost (FLOPs)\n",
      "EN-DE EN-FR EN-DE EN-FR\n",
      "ByteNet [15] 23.75\n",
      "Deep-Att + PosUnk [32] 39.2 1.0 ·1020\n",
      "GNMT + RL [31] 24.6 39.92 2.3 ·1019 1.4 ·1020\n",
      "ConvS2S [8] 25.16 40.46 9.6 ·1018 1.5 ·1020\n",
      "MoE [26] 26.03 40.56 2.0 ·1019 1.2 ·1020\n",
      "Deep-Att + PosUnk Ensemble [32] 40.4 8.0 ·1020\n",
      "GNMT + RL Ensemble [31] 26.30 41.16 1.8 ·1020 1.1 ·1021\n",
      "ConvS2S Ensemble [8] 26.36 41.29 7.7 ·1019 1.2 ·1021\n",
      "Transformer (base model) 27.3 38.1 3.3 · 1018\n",
      "Transformer (big) 28.4 41.0 2.3 ·1019\n",
      "Label Smoothing During training, we employed label smoothing of value ϵls = 0.1 [30]. This\n",
      "hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "6 Results\n",
      "6.1 Machine Translation\n",
      "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\n",
      "in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\n",
      "BLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\n",
      "listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\n",
      "surpasses all previously published models and ensembles, at a fraction of the training cost of any of\n",
      "the competitive models.\n",
      "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\n",
      "outperforming all of the previously published single models, at less than 1/4 the training cost of the\n",
      "previous state-of-the-art model. The Transformer (big) model trained for English-to-French used\n",
      "dropout rate Pdrop = 0.1, instead of 0.3.\n",
      "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which\n",
      "were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Here is the refined summary:\n",
      "\n",
      "The paper \"Attention Is All You Need\" introduces the Transformer, a novel neural network architecture that processes sequences without recurrence or convolution, relying solely on attention mechanisms. The Transformer consists of an encoder and a decoder, each composed of identical layers featuring multi-head self-attention mechanisms and position-wise fully connected feed-forward networks (FFNNs) with residual connections and layer normalization. The Transformer employs a scaled dot-product attention mechanism, which computes the output as a weighted sum of values, and multi-head attention, which computes attention in parallel on each projected version of the queries, keys, and values.\n",
      "\n",
      "The paper presents two key ideas: scaled dot-product attention and multi-head attention. Each layer in the encoder and decoder contains a position-wise fully connected feed-forward network (FFNN), which is applied to each position separately and identically. Additionally, the model uses learned embeddings to convert input and output tokens to vectors, and positional encoding is added to the input embeddings to inject information about the relative or absolute position of the tokens in the sequence.\n",
      "\n",
      "The Transformer uses a novel approach to positional encoding, where sine and cosine functions of different frequencies are used to represent the position of each token in the sequence. This allows the model to easily learn to attend by relative positions and enables it to extrapolate to sequence lengths longer than those encountered during training.\n",
      "\n",
      "The paper also discusses the computational complexity and parallelization of self-attention layers, which is more efficient than convolutional layers. Specifically, self-attention layers have a complexity of O(n), whereas convolutional layers have a complexity of O(k·n·d+ n·d2) even with k = n. Moreover, self-attention layers can be parallelized more easily, reducing the training time and making the model more scalable.\n",
      "\n",
      "The paper presents state-of-the-art results on two machine translation tasks, outperforming existing models with a new single-model state-of-the-art BLEU score of 41.0 on the WMT 2014 English-to-French translation task. The model also requires significantly less training time and is more parallelizable than previous models. The motivation behind the use of self-attention is to reduce the computational complexity per layer, increase parallelization, and shorten the path length between long-range dependencies in the network, making it easier to learn long-range dependencies.\n",
      "\n",
      "Additionally, the paper demonstrates the potential of self-attention to yield more interpretable models, with individual attention heads learning to perform different tasks and exhibiting behavior related to the syntactic and semantic structure of the sentences. The results show that the Transformer achieves better BLEU scores than previous state-of-the-art models on both English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which\n",
      "were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\n",
      "used beam search with a beam size of 4 and length penalty α= 0.6 [31]. These hyperparameters\n",
      "were chosen after experimentation on the development set. We set the maximum output length during\n",
      "inference to input length + 50, but terminate early when possible [31].\n",
      "Table 2 summarizes our results and compares our translation quality and training costs to other model\n",
      "architectures from the literature. We estimate the number of ﬂoating point operations used to train a\n",
      "model by multiplying the training time, the number of GPUs used, and an estimate of the sustained\n",
      "single-precision ﬂoating-point capacity of each GPU 5.\n",
      "6.2 Model Variations\n",
      "To evaluate the importance of different components of the Transformer, we varied our base model\n",
      "in different ways, measuring the change in performance on English-to-German translation on the\n",
      "development set, newstest2013. We used beam search as described in the previous section, but no\n",
      "checkpoint averaging. We present these results in Table 3.\n",
      "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\n",
      "keeping the amount of computation constant, as described in Section 3.2.2. While single-head\n",
      "attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
      "5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n",
      "8\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: After reviewing the additional context, I refined the summary to provide a more comprehensive overview of the paper \"Attention Is All You Need\":\n",
      "\n",
      "The paper introduces the Transformer, a novel neural network architecture that processes sequences without recurrence or convolution, relying solely on attention mechanisms. The Transformer consists of an encoder and a decoder, each composed of identical layers featuring multi-head self-attention mechanisms and position-wise fully connected feed-forward networks (FFNNs) with residual connections and layer normalization. The Transformer employs a scaled dot-product attention mechanism, which computes the output as a weighted sum of values, and multi-head attention, which computes attention in parallel on each projected version of the queries, keys, and values.\n",
      "\n",
      "The paper presents two key ideas: scaled dot-product attention and multi-head attention. Each layer in the encoder and decoder contains a position-wise fully connected feed-forward network (FFNN), which is applied to each position separately and identically. Additionally, the model uses learned embeddings to convert input and output tokens to vectors, and positional encoding is added to the input embeddings to inject information about the relative or absolute position of the tokens in the sequence.\n",
      "\n",
      "The Transformer uses a novel approach to positional encoding, where sine and cosine functions of different frequencies are used to represent the position of each token in the sequence. This allows the model to easily learn to attend by relative positions and enables it to extrapolate to sequence lengths longer than those encountered during training.\n",
      "\n",
      "The paper also discusses the computational complexity and parallelization of self-attention layers, which is more efficient than convolutional layers. Specifically, self-attention layers have a complexity of O(n), whereas convolutional layers have a complexity of O(k·n·d+ n·d2) even with k = n. Moreover, self-attention layers can be parallelized more easily, reducing the training time and making the model more scalable.\n",
      "\n",
      "The paper presents state-of-the-art results on two machine translation tasks, outperforming existing models with a new single-model state-of-the-art BLEU score of 41.0 on the WMT 2014 English-to-French translation task. The model also requires significantly less training time and is more parallelizable than previous models. The motivation behind the use of self-attention is to reduce the computational complexity per layer, increase parallelization, and shorten the path length between long-range dependencies in the network, making it easier to learn long-range dependencies.\n",
      "\n",
      "In addition, the paper explores model variations, including the impact of different attention head numbers and dimensions on English-to-German translation performance. The results show that while single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n",
      "\n",
      "The paper demonstrates the potential of self-attention to yield more interpretable models, with individual attention heads learning to perform different tasks and exhibiting behavior related to the syntactic and semantic structure of the sentences. The results show that the Transformer achieves better BLEU scores than previous state-of-the-art models on both English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\n",
      "model. All metrics are on the English-to-German translation development set, newstest2013. Listed\n",
      "perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\n",
      "per-word perplexities.\n",
      "N d model dff h d k dv Pdrop ϵls\n",
      "train PPL BLEU params\n",
      "steps (dev) (dev) ×106\n",
      "base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n",
      "(A)\n",
      "1 512 512 5.29 24.9\n",
      "4 128 128 5.00 25.5\n",
      "16 32 32 4.91 25.8\n",
      "32 16 16 5.01 25.4\n",
      "(B) 16 5.16 25.1 58\n",
      "32 5.01 25.4 60\n",
      "(C)\n",
      "2 6.11 23.7 36\n",
      "4 5.19 25.3 50\n",
      "8 4.88 25.5 80\n",
      "256 32 32 5.75 24.5 28\n",
      "1024 128 128 4.66 26.0 168\n",
      "1024 5.12 25.4 53\n",
      "4096 4.75 26.2 90\n",
      "(D)\n",
      "0.0 5.77 24.6\n",
      "0.2 4.95 25.5\n",
      "0.0 4.67 25.3\n",
      "0.2 5.47 25.7\n",
      "(E) positional embedding instead of sinusoids 4.92 25.7\n",
      "big 6 1024 4096 16 0.3 300K 4.33 26.4 213\n",
      "In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\n",
      "suggests that determining compatibility is not easy and that a more sophisticated compatibility\n",
      "function than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\n",
      "bigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\n",
      "sinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\n",
      "results to the base model.\n",
      "7 Conclusion\n",
      "In this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\n",
      "attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\n",
      "multi-headed self-attention.\n",
      "For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\n",
      "on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\n",
      "English-to-French translation tasks, we achieve a new state of the art. In the former task our best\n",
      "model outperforms even all previously reported ensembles.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Based on the provided context, I refined the summary to provide a more comprehensive overview of the paper \"Attention Is All You Need\".\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "English-to-French translation tasks, we achieve a new state of the art. In the former task our best\n",
      "model outperforms even all previously reported ensembles.\n",
      "We are excited about the future of attention-based models and plan to apply them to other tasks. We\n",
      "plan to extend the Transformer to problems involving input and output modalities other than text and\n",
      "to investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\n",
      "such as images, audio and video. Making generation less sequential is another research goals of ours.\n",
      "The code we used to train and evaluate our models is available at https://github.com/\n",
      "tensorflow/tensor2tensor.\n",
      "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\n",
      "comments, corrections and inspiration.\n",
      "9\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Based on the provided context, I refine the summary as follows:\n",
      "\n",
      "The paper \"Attention Is All You Need\" presents a groundbreaking achievement in English-to-French translation tasks, surpassing previous state-of-the-art results and even outperforming ensembles of previous models. The authors are enthusiastic about the potential of attention-based models and plan to expand their research to other tasks, including those involving input and output modalities beyond text, such as images, audio, and video. They also aim to develop local, restricted attention mechanisms to efficiently handle large inputs and outputs. Furthermore, they plan to investigate ways to make generation less sequential. The authors provide access to the code used to train and evaluate their models on GitHub and acknowledge the contributions of Nal Kalchbrenner and Stephan Gouws.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "References\n",
      "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\n",
      "arXiv:1607.06450, 2016.\n",
      "[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\n",
      "learning to align and translate. CoRR, abs/1409.0473, 2014.\n",
      "[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\n",
      "machine translation architectures. CoRR, abs/1703.03906, 2017.\n",
      "[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\n",
      "reading. arXiv preprint arXiv:1601.06733, 2016.\n",
      "[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\n",
      "and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\n",
      "machine translation. CoRR, abs/1406.1078, 2014.\n",
      "[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\n",
      "preprint arXiv:1610.02357, 2016.\n",
      "[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\n",
      "of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n",
      "[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\n",
      "tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n",
      "[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\n",
      "arXiv:1308.0850, 2013.\n",
      "[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\n",
      "age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\n",
      "Recognition, pages 770–778, 2016.\n",
      "[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\n",
      "recurrent nets: the difﬁculty of learning long-term dependencies, 2001.\n",
      "[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\n",
      "9(8):1735–1780, 1997.\n",
      "[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The original summary remains unchanged, as the provided context does not seem to add any new information that would significantly modify the summary. The references provided appear to be a list of papers and research articles related to neural machine translation, attention mechanisms, and recurrent neural networks, which do not directly impact the main points of the original summary.\n",
      "\n",
      "Therefore, the refined summary remains the same as the original:\n",
      "\n",
      "The paper \"Attention Is All You Need\" presents a groundbreaking achievement in English-to-French translation tasks, surpassing previous state-of-the-art results and even outperforming ensembles of previous models. The authors are enthusiastic about the potential of attention-based models and plan to expand their research to other tasks, including those involving input and output modalities beyond text, such as images, audio, and video. They also aim to develop local, restricted attention mechanisms to efficiently handle large inputs and outputs. Furthermore, they plan to investigate ways to make generation less sequential. The authors provide access to the code used to train and evaluate their models on GitHub and acknowledge the contributions of Nal Kalchbrenner and Stephan Gouws.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "9(8):1735–1780, 1997.\n",
      "[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\n",
      "the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n",
      "[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\n",
      "on Learning Representations (ICLR), 2016.\n",
      "[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\n",
      "ray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\n",
      "2017.\n",
      "[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\n",
      "In International Conference on Learning Representations, 2017.\n",
      "[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n",
      "[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\n",
      "arXiv:1703.10722, 2017.\n",
      "[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\n",
      "Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\n",
      "arXiv:1703.03130, 2017.\n",
      "[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\n",
      "Information Processing Systems, (NIPS), 2016.\n",
      "10\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: Based on the provided context, the original summary remains unchanged. The list of papers and research articles related to neural machine translation, attention mechanisms, and recurrent neural networks does not provide any new information that would significantly modify the summary.\n",
      "\n",
      "The original summary remains the same:\n",
      "\n",
      "\"The paper 'Attention Is All You Need' presents a groundbreaking achievement in English-to-French translation tasks, surpassing previous state-of-the-art results and even outperforming ensembles of previous models. The authors are enthusiastic about the potential of attention-based models and plan to expand their research to other tasks, including those involving input and output modalities beyond text, such as images, audio, and video. They also aim to develop local, restricted attention mechanisms to efficiently handle large inputs and outputs. Furthermore, they plan to investigate ways to make generation less sequential. The authors provide access to the code used to train and evaluate their models on GitHub and acknowledge the contributions of Nal Kalchbrenner and Stephan Gouws.\"\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\n",
      "based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n",
      "[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\n",
      "model. In Empirical Methods in Natural Language Processing, 2016.\n",
      "[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\n",
      "summarization. arXiv preprint arXiv:1705.04304, 2017.\n",
      "[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\n",
      "preprint arXiv:1608.05859, 2016.\n",
      "[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\n",
      "with subword units. arXiv preprint arXiv:1508.07909, 2015.\n",
      "[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\n",
      "and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\n",
      "layer. arXiv preprint arXiv:1701.06538, 2017.\n",
      "[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\n",
      "nov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\n",
      "Learning Research, 15(1):1929–1958, 2014.\n",
      "[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\n",
      "networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\n",
      "Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\n",
      "Inc., 2015.\n",
      "[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\n",
      "networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\n",
      "[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\n",
      "Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n",
      "[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The original summary remains the same:\n",
      "\n",
      "\"The paper 'Attention Is All You Need' presents a groundbreaking achievement in English-to-French translation tasks, surpassing previous state-of-the-art results and even outperforming ensembles of previous models. The authors are enthusiastic about the potential of attention-based models and plan to expand their research to other tasks, including those involving input and output modalities beyond text, such as images, audio, and video. They also aim to develop local, restricted attention mechanisms to efficiently handle large inputs and outputs. Furthermore, they plan to investigate ways to make generation less sequential. The authors provide access to the code used to train and evaluate their models on GitHub and acknowledge the contributions of Nal Kalchbrenner and Stephan Gouws.\"\n",
      "\n",
      "The new context provides a list of research papers and articles related to neural machine translation, attention mechanisms, and recurrent neural networks. However, these papers do not provide new information that would significantly modify the original summary. Therefore, the original summary remains unchanged.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n",
      "[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\n",
      "Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\n",
      "translation system: Bridging the gap between human and machine translation. arXiv preprint\n",
      "arXiv:1609.08144, 2016.\n",
      "[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\n",
      "fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n",
      "11\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The original summary remains unchanged:\\n\\n\"The paper \\'Attention Is All You Need\\' presents a groundbreaking achievement in English-to-French translation tasks, surpassing previous state-of-the-art results and even outperforming ensembles of previous models. The authors are enthusiastic about the potential of attention-based models and plan to expand their research to other tasks, including those involving input and output modalities beyond text, such as images, audio, and video. They also aim to develop local, restricted attention mechanisms to efficiently handle large inputs and outputs. Furthermore, they plan to investigate ways to make generation less sequential. The authors provide access to the code used to train and evaluate their models on GitHub and acknowledge the contributions of Nal Kalchbrenner and Stephan Gouws.\"\\n\\nThe provided research papers and articles do not provide new information that would significantly modify the original summary, so it remains the same.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_chain = load_summarize_chain(llm = llm,\n",
    "                                     chain_type = 'refine',\n",
    "                                     verbose = True\n",
    "                                     )\n",
    "summary = summary_chain.run(docs)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
